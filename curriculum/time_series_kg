// ═══════════════════════════════════════════════════════════════════════════════
// TIME SERIES END-TO-END PREDICTION KNOWLEDGE GRAPH
// For Neo4j — covers data handling, EDA, preprocessing, modelling,
// prediction types, evaluation, concepts, considerations, do's & don'ts
// ═══════════════════════════════════════════════════════════════════════════════

// ─────────────────────────────────────────────────────────────────────────────
// PIPELINE STAGE NODES  (the backbone — ordered stages in an E2E pipeline)
// ─────────────────────────────────────────────────────────────────────────────
CREATE (stageIngest:PipelineStage       { name: "Data Ingestion",           order: 1,  description: "Collecting raw time series data from various sources" })
CREATE (stageEDA:PipelineStage          { name: "Exploratory Data Analysis", order: 2,  description: "Understanding structure, quality, and patterns in the time series" })
CREATE (stagePreprocess:PipelineStage   { name: "Preprocessing",             order: 3,  description: "Cleaning, aligning, and transforming data for modelling" })
CREATE (stageSampling:PipelineStage     { name: "Sampling & Windowing",      order: 4,  description: "Creating supervised learning samples from sequential data" })
CREATE (stageAugment:PipelineStage      { name: "Data Augmentation",         order: 5,  description: "Expanding training set to improve generalisation" })
CREATE (stageFeature:PipelineStage      { name: "Feature Engineering",       order: 6,  description: "Deriving informative features from raw time series" })
CREATE (stageModel:PipelineStage        { name: "Modelling",                 order: 7,  description: "Selecting, building, and training prediction models" })
CREATE (stageEval:PipelineStage         { name: "Evaluation",                order: 8,  description: "Measuring model performance with appropriate metrics" })
CREATE (stageDeploy:PipelineStage       { name: "Deployment & Monitoring",   order: 9,  description: "Serving predictions and monitoring for drift in production" })

// ─────────────────────────────────────────────────────────────────────────────
// CONCEPT NODES
// ─────────────────────────────────────────────────────────────────────────────
CREATE (cStationarity:Concept {
    name: "Stationarity",
    description: "A stationary time series has constant mean, variance, and autocovariance over time. Most classical models assume stationarity.",
    types: ["Strict stationarity", "Weak (covariance) stationarity"],
    why_it_matters: "Non-stationary data causes spurious regressions and invalid statistical inference"
})
CREATE (cTrend:Concept {
    name: "Trend",
    description: "Long-term systematic increase or decrease in the level of a time series",
    types: ["Linear trend", "Nonlinear trend", "Stochastic trend (unit root)"],
    why_it_matters: "Unremoved trend violates stationarity and inflates model error"
})
CREATE (cSeasonality:Concept {
    name: "Seasonality",
    description: "Periodic, predictable fluctuations occurring at fixed known intervals (daily, weekly, annual)",
    types: ["Additive seasonality", "Multiplicative seasonality"],
    why_it_matters: "Ignored seasonality leads to systematic forecast bias"
})
CREATE (cCyclicality:Concept {
    name: "Cyclicality",
    description: "Long-period fluctuations NOT of fixed frequency, driven by economic or systemic forces. Distinct from seasonality.",
    why_it_matters: "Cyclic patterns require longer historical data to learn and are harder to forecast"
})
CREATE (cNoise:Concept {
    name: "Irregular / Noise",
    description: "Random, unpredictable residual variation after removing trend, seasonality, and cyclicality",
    why_it_matters: "Distinguishing signal from noise prevents overfitting"
})
CREATE (cAutocorrelation:Concept {
    name: "Autocorrelation",
    description: "Correlation of a time series with its own past values at various lags",
    why_it_matters: "Quantifies temporal dependency; guides lag selection for AR models and features"
})
CREATE (cPartialAutocorrelation:Concept {
    name: "Partial Autocorrelation",
    description: "Correlation between a series and its lag after removing the effect of shorter lags",
    why_it_matters: "Identifies direct lag relationships; used to determine AR order p"
})
CREATE (cDataLeakage:Concept {
    name: "Data Leakage",
    description: "Future information inadvertently used during training, causing inflated validation scores that do not hold in production",
    types: ["Target leakage", "Temporal leakage", "Feature leakage from test set statistics"],
    why_it_matters: "Most common cause of overly optimistic time series models"
})
CREATE (cCovariate:Concept {
    name: "Exogenous Variables / Covariates",
    description: "External variables that influence the target series but are not predicted by the model (e.g., temperature, promotions)",
    why_it_matters: "Can significantly improve forecast accuracy when correlated with the target"
})
CREATE (cMultivariate:Concept {
    name: "Multivariate Time Series",
    description: "Multiple interdependent time series observed simultaneously (e.g., sensor arrays)",
    why_it_matters: "Requires joint modelling to capture cross-variable dependencies"
})
CREATE (cIrregularSampling:Concept {
    name: "Irregular Sampling",
    description: "Time series where observations are not equally spaced in time",
    why_it_matters: "Most standard models require regular intervals; requires resampling or specialised methods"
})
CREATE (cLookbackWindow:Concept {
    name: "Lookback Window",
    description: "The number of past time steps fed as input to the model for each prediction",
    why_it_matters: "Too short loses context; too long adds noise and increases computation"
})
CREATE (cHorizon:Concept {
    name: "Forecast Horizon",
    description: "How many future steps ahead the model predicts",
    types: ["Single-step (h=1)", "Multi-step (h>1)"],
    why_it_matters: "Longer horizons accumulate error; strategy choice depends on horizon"
})
CREATE (cConceptDrift:Concept {
    name: "Concept Drift",
    description: "Change in the statistical properties of the target variable over time, making historical patterns less relevant",
    types: ["Sudden drift", "Gradual drift", "Recurring drift", "Incremental drift"],
    why_it_matters: "Deployed models degrade silently without drift monitoring and retraining"
})
CREATE (cUnitRoot:Concept {
    name: "Unit Root",
    description: "A characteristic of non-stationary processes where shocks have permanent effects",
    why_it_matters: "Unit root processes must be differenced to achieve stationarity for ARIMA-family models"
})
CREATE (cClassImbalance:Concept {
    name: "Class Imbalance in Time Series",
    description: "Rare events (faults, anomalies, failures) are vastly outnumbered by normal-operation samples",
    why_it_matters: "Models default to predicting majority class; precision/recall on minority class is what matters"
})
CREATE (cCausality:Concept {
    name: "Granger Causality",
    description: "A statistical concept: series X Granger-causes Y if past values of X improve prediction of Y beyond Y's own past",
    why_it_matters: "Helps select relevant exogenous features and understand system dynamics"
})
CREATE (cRUL:Concept {
    name: "Remaining Useful Life (RUL)",
    description: "The estimated time remaining before a component or system fails under current operating conditions",
    why_it_matters: "Core target for predictive maintenance; requires monotonically constrained regression models"
})
CREATE (cChangePoint:Concept {
    name: "Change Point",
    description: "A time point where the statistical properties of the series abruptly change (mean shift, variance change)",
    why_it_matters: "Undetected change points corrupt model training; must be segmented or modelled explicitly"
})
CREATE (cAnomalyTypes:Concept {
    name: "Anomaly Types",
    description: "Classification of anomalies in time series",
    types: ["Point anomaly (single outlier)", "Contextual anomaly (outlier given context)", "Collective anomaly (sequence is anomalous)"],
    why_it_matters: "Different types require different detection strategies"
})
CREATE (cTemporalDependency:Concept {
    name: "Temporal Dependency",
    description: "The property that observations close in time are more correlated than observations far apart",
    why_it_matters: "Violates i.i.d. assumption of classical ML; requires temporal-aware train/test splits"
})
CREATE (cMulticollinearity:Concept {
    name: "Multicollinearity in Lagged Features",
    description: "High correlation between lagged versions of the same series used as features",
    why_it_matters: "Inflates regression coefficients and reduces model interpretability"
})
CREATE (cNormalization:Concept {
    name: "Normalization / Scaling",
    description: "Transforming feature values to a common scale to prevent features with large magnitudes dominating",
    types: ["Min-Max scaling", "Z-score standardisation", "Robust scaling", "Instance normalisation"],
    why_it_matters: "Especially critical for RNNs and distance-based models; must be fit only on training data"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — EDA
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tTimeplot:Technique {
    name: "Time Series Line Plot",
    use_case: "First visual inspection of raw series — trend, seasonality, anomalies",
    limitation: "Hard to interpret when many series are overlaid",
    python_function: "df.plot() or plt.plot(df.index, df['value'])",
    stage: "EDA"
})
CREATE (tACFPlot:Technique {
    name: "ACF Plot",
    use_case: "Reveal seasonality lags and MA order q for ARIMA",
    limitation: "Affected by trend; difference first before plotting",
    python_function: "plot_acf(series, lags=40) from statsmodels",
    stage: "EDA"
})
CREATE (tPACFPlot:Technique {
    name: "PACF Plot",
    use_case: "Identify direct AR order p for ARIMA",
    limitation: "Only valid for stationary series",
    python_function: "plot_pacf(series, lags=40) from statsmodels",
    stage: "EDA"
})
CREATE (tADFTest:Technique {
    name: "Augmented Dickey-Fuller Test",
    use_case: "Test for unit root / non-stationarity",
    limitation: "Low power on short series; struggles with structural breaks",
    python_function: "adfuller(series) from statsmodels.tsa.stattools",
    stage: "EDA"
})
CREATE (tKPSSTest:Technique {
    name: "KPSS Test",
    use_case: "Complement to ADF — null hypothesis is stationarity (opposite of ADF)",
    limitation: "Sensitive to lag window selection",
    python_function: "kpss(series) from statsmodels.tsa.stattools",
    stage: "EDA"
})
CREATE (tSeasonalDecomp:Technique {
    name: "Seasonal Decomposition",
    use_case: "Visually separate trend, seasonal, and residual components",
    limitation: "Additive decomposition assumes constant seasonal amplitude",
    python_function: "seasonal_decompose(series, model='additive') from statsmodels",
    stage: "EDA"
})
CREATE (tSTL:Technique {
    name: "STL Decomposition",
    use_case: "Robust seasonal decomposition handling outliers and changing seasonality",
    limitation: "Seasonal period must be specified; computationally heavier than classical decomp",
    python_function: "STL(series, period=12).fit() from statsmodels",
    stage: "EDA"
})
CREATE (tRollingStats:Technique {
    name: "Rolling Mean & Std Dev",
    use_case: "Visual stationarity check; detect variance changes over time",
    limitation: "Window size choice affects interpretation",
    python_function: "df.rolling(window=12).mean(), df.rolling(window=12).std()",
    stage: "EDA"
})
CREATE (tLagPlot:Technique {
    name: "Lag Plot",
    use_case: "Visualise autocorrelation structure at a specific lag",
    limitation: "Shows only one lag at a time",
    python_function: "pd.plotting.lag_plot(series, lag=1)",
    stage: "EDA"
})
CREATE (tOutlierIQR:Technique {
    name: "IQR Outlier Detection",
    use_case: "Flag statistical outliers beyond 1.5×IQR fence",
    limitation: "Not temporal-context-aware; contextual anomalies may pass undetected",
    python_function: "Q1, Q3 = series.quantile([0.25, 0.75]); IQR = Q3-Q1",
    stage: "EDA"
})
CREATE (tZScore:Technique {
    name: "Z-Score Outlier Detection",
    use_case: "Flag values more than N standard deviations from the mean",
    limitation: "Sensitive to outliers in mean/std estimation; assumes normality",
    python_function: "zscore(series) from scipy.stats; flag where |z|>3",
    stage: "EDA"
})
CREATE (tMissingPattern:Technique {
    name: "Missing Value Pattern Analysis",
    use_case: "Identify MCAR, MAR, MNAR patterns and gap lengths",
    limitation: "MNAR cannot be verified from data alone",
    python_function: "msno.matrix(df) from missingno library",
    stage: "EDA"
})
CREATE (tHistogram:Technique {
    name: "Histogram / KDE Plot",
    use_case: "Understand value distribution, skewness, bimodality",
    limitation: "Does not capture temporal ordering",
    python_function: "sns.histplot(series, kde=True)",
    stage: "EDA"
})
CREATE (tCorrelogram:Technique {
    name: "Cross-Correlation Plot",
    use_case: "Measure lag relationship between two different time series",
    limitation: "Spurious correlations possible with non-stationary series",
    python_function: "plt.xcorr(series1, series2, maxlags=30)",
    stage: "EDA"
})
CREATE (tGrangerTest:Technique {
    name: "Granger Causality Test",
    use_case: "Determine if one series helps predict another",
    limitation: "Statistical causality only; does not imply physical causality",
    python_function: "grangercausalitytests(data, maxlag=5) from statsmodels",
    stage: "EDA"
})
CREATE (tChangePointDetect:Technique {
    name: "Change Point Detection",
    use_case: "Locate structural breaks in mean or variance",
    limitation: "Multiple algorithms disagree; hyperparameter sensitive",
    python_function: "ruptures library: rpt.Pelt().fit(signal).predict(pen=3)",
    stage: "EDA"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — PREPROCESSING
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tResample:Technique {
    name: "Resampling / Frequency Alignment",
    use_case: "Convert irregular or multi-frequency series to uniform intervals",
    limitation: "Upsampling creates artificial values; downsampling loses detail",
    python_function: "df.resample('1H').mean() or .interpolate()",
    stage: "Preprocessing"
})
CREATE (tDifferencing:Technique {
    name: "Differencing",
    use_case: "Remove trend and achieve stationarity",
    limitation: "Over-differencing inflates variance; seasonal differencing requires period knowledge",
    python_function: "series.diff(1) for first-order; series.diff(12) for seasonal",
    stage: "Preprocessing"
})
CREATE (tLogTransform:Technique {
    name: "Log Transform",
    use_case: "Stabilise multiplicative variance; compress range of exponential growth series",
    limitation: "Fails on zero or negative values; requires back-transformation",
    python_function: "np.log1p(series) — safer than np.log for near-zero values",
    stage: "Preprocessing"
})
CREATE (tBoxCox:Technique {
    name: "Box-Cox Transform",
    use_case: "Parametric variance-stabilising transformation that includes log as special case",
    limitation: "Requires positive values; lambda must be stored for back-transform",
    python_function: "boxcox(series) from scipy.stats",
    stage: "Preprocessing"
})
CREATE (tMinMax:Technique {
    name: "Min-Max Scaling",
    use_case: "Scale features to [0,1]; required for LSTM/RNN input stability",
    limitation: "Sensitive to outliers; test/live data may exceed training range",
    python_function: "MinMaxScaler().fit_transform(X_train) from sklearn",
    stage: "Preprocessing"
})
CREATE (tZScaleStd:Technique {
    name: "Z-Score Standardisation",
    use_case: "Zero mean, unit variance scaling; appropriate when distribution is roughly normal",
    limitation: "Outliers distort mean and std; robust alternative uses median/IQR",
    python_function: "StandardScaler().fit_transform(X_train) from sklearn",
    stage: "Preprocessing"
})
CREATE (tRobustScaler:Technique {
    name: "Robust Scaling",
    use_case: "Scale using median and IQR; robust to outliers in sensor/industrial data",
    limitation: "Does not bound values to a fixed range",
    python_function: "RobustScaler().fit_transform(X_train) from sklearn",
    stage: "Preprocessing"
})
CREATE (tSmoothingMA:Technique {
    name: "Moving Average Smoothing",
    use_case: "Reduce high-frequency noise to reveal underlying trend",
    limitation: "Introduces lag; removes peaks that might be important anomalies",
    python_function: "series.rolling(window=7).mean()",
    stage: "Preprocessing"
})
CREATE (tEWMA:Technique {
    name: "Exponentially Weighted Moving Average",
    use_case: "Noise smoothing with more weight on recent values than simple MA",
    limitation: "Alpha parameter requires tuning; still introduces lag",
    python_function: "series.ewm(span=7).mean()",
    stage: "Preprocessing"
})
CREATE (tSavitzky:Technique {
    name: "Savitzky-Golay Filter",
    use_case: "Polynomial smoothing that better preserves peak shape than MA",
    limitation: "Window length and polynomial order must be chosen carefully",
    python_function: "savgol_filter(series, window_length=11, polyorder=2) from scipy",
    stage: "Preprocessing"
})
CREATE (tOutlierClip:Technique {
    name: "Outlier Clipping / Winsorisation",
    use_case: "Cap extreme values at percentile bounds to limit their influence",
    limitation: "Information loss; may mask real fault signatures",
    python_function: "series.clip(lower=series.quantile(0.01), upper=series.quantile(0.99))",
    stage: "Preprocessing"
})
CREATE (tDetrending:Technique {
    name: "Detrending",
    use_case: "Remove deterministic trend via regression subtraction or differencing",
    limitation: "OLS detrending assumes linear trend; stochastic trends require differencing",
    python_function: "detrend(series) from scipy.signal or manual OLS regression",
    stage: "Preprocessing"
})
CREATE (tDeseasoning:Technique {
    name: "Deseasoning",
    use_case: "Remove seasonal component using decomposition residuals or seasonal differencing",
    limitation: "Requires known seasonality period; multiple periods require STL",
    python_function: "series.diff(seasonal_period)",
    stage: "Preprocessing"
})
CREATE (tTimeIndexAlign:Technique {
    name: "Time Index Alignment",
    use_case: "Synchronise multiple sensors / data sources to a common timestamp index",
    limitation: "Misaligned clocks introduce systematic errors",
    python_function: "pd.merge_asof() for nearest-timestamp join",
    stage: "Preprocessing"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — IMPUTATION
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tFFill:Technique {
    name: "Forward Fill (Last Observation Carried Forward)",
    use_case: "Sensor data where last measurement persists until next reading",
    limitation: "Introduces lag bias; dangerous for rapidly changing signals",
    python_function: "df.ffill()",
    stage: "Imputation"
})
CREATE (tBFill:Technique {
    name: "Backward Fill",
    use_case: "When future value is known to be correct reference (rare in real-time)",
    limitation: "Constitutes data leakage if used in causal forecasting",
    python_function: "df.bfill()",
    stage: "Imputation"
})
CREATE (tLinInterp:Technique {
    name: "Linear Interpolation",
    use_case: "Short gaps in smooth continuous signals",
    limitation: "Assumes linearity between known points; poor for abrupt changes",
    python_function: "df.interpolate(method='linear')",
    stage: "Imputation"
})
CREATE (tSplineInterp:Technique {
    name: "Spline Interpolation",
    use_case: "Longer gaps in smooth nonlinear signals",
    limitation: "Risk of Runge's phenomenon for high-order splines at gap edges",
    python_function: "df.interpolate(method='spline', order=3)",
    stage: "Imputation"
})
CREATE (tKalmanImpute:Technique {
    name: "Kalman Filter Imputation",
    use_case: "Noisy sensor streams; handles structural state-space models",
    limitation: "Requires model parameterisation; computationally intensive",
    python_function: "KalmanFilter().em(X).smooth(X) from pykalman",
    stage: "Imputation"
})
CREATE (tMICE:Technique {
    name: "MICE (Multiple Imputation by Chained Equations)",
    use_case: "When missingness depends on other observed variables (MAR); multivariate",
    limitation: "Ignores temporal structure; each imputed dataset needs separate model",
    python_function: "IterativeImputer() from sklearn.impute",
    stage: "Imputation"
})
CREATE (tSeasonalImpute:Technique {
    name: "Seasonal Mean Imputation",
    use_case: "Replace missing value with mean of same season/period from other years",
    limitation: "Assumes stable seasonality; requires sufficient historical seasons",
    python_function: "Custom: mean of same day-of-week / month across years",
    stage: "Imputation"
})
CREATE (tGANImpute:Technique {
    name: "GAN-Based Imputation (GAIN / GRUI)",
    use_case: "Long gaps in complex multivariate time series",
    limitation: "Training instability; requires large datasets; hard to interpret",
    python_function: "GAIN paper implementation or tsai library",
    stage: "Imputation"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — SAMPLING & WINDOWING
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tSlidingWindow:Technique {
    name: "Sliding Window",
    use_case: "Convert time series to supervised (X, y) pairs with fixed lookback",
    limitation: "Adjacent windows are highly correlated; can cause information leakage if not handled",
    python_function: "np.lib.stride_tricks.sliding_window_view(arr, window_shape=W)",
    stage: "Sampling"
})
CREATE (tTumblingWindow:Technique {
    name: "Tumbling (Non-Overlapping) Window",
    use_case: "Independent windows for classification; reduces data redundancy",
    limitation: "Smaller effective dataset size; may miss events at window boundaries",
    python_function: "df.groupby(df.index // W)",
    stage: "Sampling"
})
CREATE (tExpandingWindow:Technique {
    name: "Expanding Window (Walk-Forward) CV",
    use_case: "Temporal cross-validation that respects time order for honest evaluation",
    limitation: "Early folds have very few training samples",
    python_function: "TimeSeriesSplit(n_splits=5) from sklearn",
    stage: "Sampling"
})
CREATE (tSlidingWindowCV:Technique {
    name: "Sliding Window Cross-Validation",
    use_case: "Rolling CV with fixed training size; simulates production retraining",
    limitation: "More expensive than single train/test split",
    python_function: "TimeSeriesSplit(n_splits=5, max_train_size=N)",
    stage: "Sampling"
})
CREATE (tStratifiedTemporal:Technique {
    name: "Stratified Temporal Split",
    use_case: "Maintain class balance in train/val/test while preserving temporal order",
    limitation: "Difficult when rare events are clustered in time",
    python_function: "Custom split: sort by time then sample strata proportionally",
    stage: "Sampling"
})
CREATE (tDownsample:Technique {
    name: "Temporal Downsampling",
    use_case: "Reduce data volume or match target frequency from high-rate sensors",
    limitation: "Aliasing risk if Nyquist criterion not met",
    python_function: "df.resample('10min').mean()",
    stage: "Sampling"
})
CREATE (tUpsample:Technique {
    name: "Temporal Upsampling",
    use_case: "Align low-frequency series with high-frequency target",
    limitation: "Creates synthetic values; must be followed by interpolation",
    python_function: "df.resample('1min').interpolate()",
    stage: "Sampling"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — DATA AUGMENTATION
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tJittering:Technique {
    name: "Jittering (Gaussian Noise Addition)",
    use_case: "Increase robustness by simulating sensor noise",
    limitation: "Excessive noise destroys signal; std must be calibrated to SNR",
    python_function: "series + np.random.normal(0, sigma, len(series))",
    stage: "Augmentation"
})
CREATE (tScaling:Technique {
    name: "Magnitude Scaling",
    use_case: "Simulate amplitude variation between machines or operating modes",
    limitation: "Does not change temporal pattern; limited diversity gain",
    python_function: "series * np.random.uniform(0.8, 1.2)",
    stage: "Augmentation"
})
CREATE (tTimeWarp:Technique {
    name: "Time Warping",
    use_case: "Simulate speed variation in repeated activities (gait, assembly line cycles)",
    limitation: "Label alignment required if warping classification boundaries",
    python_function: "tsaug library: TimeWarp().augment(X)",
    stage: "Augmentation"
})
CREATE (tWindowSlice:Technique {
    name: "Window Slicing",
    use_case: "Extract multiple sub-sequences from one training sample",
    limitation: "Short slices may lose critical long-range patterns",
    python_function: "tsaug library: Crop(size=0.8).augment(X)",
    stage: "Augmentation"
})
CREATE (tDTWBaryAug:Technique {
    name: "DTW Barycentric Averaging",
    use_case: "Generate synthetic samples as the DTW average of multiple real samples",
    limitation: "Computationally expensive; softens sharp transitions",
    python_function: "dtaidistance library: dtw_barycenter_averaging(samples)",
    stage: "Augmentation"
})
CREATE (tGANAug:Technique {
    name: "GAN-Based Augmentation (TimeGAN / TGAN)",
    use_case: "Generate synthetic realistic time series for rare-class oversampling",
    limitation: "Requires large real dataset to train the GAN; mode collapse risk",
    python_function: "ydata-synthetic library: TimeGAN",
    stage: "Augmentation"
})
CREATE (tFreqAug:Technique {
    name: "Frequency Domain Augmentation",
    use_case: "Perturb FFT magnitude/phase to generate diverse synthetic samples",
    limitation: "Inverse FFT may produce unrealistic transients",
    python_function: "np.fft.fft(series); perturb; np.fft.ifft()",
    stage: "Augmentation"
})
CREATE (tMixup:Technique {
    name: "Time Series Mixup",
    use_case: "Linear interpolation between two training samples and their labels",
    limitation: "Ambiguous label interpolation for classification; may not preserve physical meaning",
    python_function: "lambda * x1 + (1-lambda) * x2; label = lambda * y1 + (1-lambda) * y2",
    stage: "Augmentation"
})
CREATE (tSMOTETS:Technique {
    name: "SMOTE for Time Series",
    use_case: "Oversample minority class windows using interpolation in window-feature space",
    limitation: "Ignores temporal structure within each window",
    python_function: "SMOTE() from imblearn applied to flattened window features",
    stage: "Augmentation"
})

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE NODES — FEATURE ENGINEERING
// ─────────────────────────────────────────────────────────────────────────────
CREATE (tLagFeatures:Technique {
    name: "Lag Features",
    use_case: "Provide model with explicit past values as input features",
    limitation: "High multicollinearity between adjacent lags; feature selection needed",
    python_function: "df['lag1'] = df['value'].shift(1)",
    stage: "Feature Engineering"
})
CREATE (tRollingFeatures:Technique {
    name: "Rolling Statistical Features",
    use_case: "Capture local temporal statistics: rolling mean, std, min, max, skew, kurt",
    limitation: "Window boundary choice is arbitrary; must be excluded from look-ahead",
    python_function: "df.rolling(W).agg(['mean','std','min','max','skew'])",
    stage: "Feature Engineering"
})
CREATE (tEWMAFeatures:Technique {
    name: "EWM Features",
    use_case: "Exponentially-weighted statistics that weight recent observations more",
    limitation: "Multiple span values increase feature count; selection needed",
    python_function: "df.ewm(span=12).mean()",
    stage: "Feature Engineering"
})
CREATE (tFFTFeatures:Technique {
    name: "FFT / Spectral Features",
    use_case: "Frequency-domain energy, dominant frequency, spectral centroid for vibration/acoustic data",
    limitation: "Non-stationary signals need STFT; loses temporal localisation",
    python_function: "np.fft.fft(window); np.abs(fft[:N//2])",
    stage: "Feature Engineering"
})
CREATE (tWavelet:Technique {
    name: "Wavelet Transform Features",
    use_case: "Time-frequency features; captures both timing and frequency of transients",
    limitation: "Choice of wavelet family and decomposition level requires domain knowledge",
    python_function: "pywt.wavedec(signal, 'db4', level=4)",
    stage: "Feature Engineering"
})
CREATE (tCalendarFeatures:Technique {
    name: "Calendar / Temporal Features",
    use_case: "Encode hour-of-day, day-of-week, month, quarter, holidays as model inputs",
    limitation: "Cyclical features need sin/cos encoding to avoid 23 hour > 0 hour artefact",
    python_function: "df['hour_sin'] = np.sin(2*np.pi*df.index.hour/24)",
    stage: "Feature Engineering"
})
CREATE (tTsfresh:Technique {
    name: "tsfresh Automated Feature Extraction",
    use_case: "Automatically extract 700+ statistical features per time series window",
    limitation: "Computationally expensive; produces highly correlated features; needs selection",
    python_function: "tsfresh.extract_features(df, column_id='id', column_sort='time')",
    stage: "Feature Engineering"
})
CREATE (tCumulative:Technique {
    name: "Cumulative / Integrated Features",
    use_case: "Cumulative sum / count since last event; useful for RUL and degradation tracking",
    limitation: "Can create unit-root-like non-stationarity; normalise cycle by cycle",
    python_function: "df['cum_cycles'] = df.groupby('engine_id').cumcount()",
    stage: "Feature Engineering"
})
CREATE (tTargetEncode:Technique {
    name: "Target Encoding of Categorical Covariates",
    use_case: "Encode high-cardinality categoricals (machine ID, location) using target statistics",
    limitation: "Data leakage if not computed within CV fold on training data only",
    python_function: "category_encoders.TargetEncoder()",
    stage: "Feature Engineering"
})

// ─────────────────────────────────────────────────────────────────────────────
// MODEL NODES — STATISTICAL / CLASSICAL
// ─────────────────────────────────────────────────────────────────────────────
CREATE (mAR:Model {
    name: "AR (Autoregressive)",
    family: "Statistical",
    description: "Regresses series on its own past p values",
    strengths: "Simple; interpretable; fast",
    weaknesses: "Linear only; assumes stationarity; no exogenous inputs",
    hyperparameters: ["p (AR order)"],
    python_class: "AutoReg(series, lags=p) from statsmodels"
})
CREATE (mMA:Model {
    name: "MA (Moving Average)",
    family: "Statistical",
    description: "Models series as function of past q forecast errors",
    strengths: "Captures short-memory processes; invertible",
    weaknesses: "Cannot model long-memory; standalone rarely used",
    hyperparameters: ["q (MA order)"],
    python_class: "ARMA(series, order=(0,q)) from statsmodels"
})
CREATE (mARIMA:Model {
    name: "ARIMA",
    family: "Statistical",
    description: "AR + Integrated (differencing) + MA. Standard workhorse for univariate forecasting.",
    strengths: "Well-understood; auto_arima for order selection",
    weaknesses: "Assumes linearity; no seasonality without extension; struggles beyond ~100 step horizon",
    hyperparameters: ["p", "d (differencing order)", "q"],
    python_class: "ARIMA(series, order=(p,d,q)) from statsmodels"
})
CREATE (mSARIMA:Model {
    name: "SARIMA",
    family: "Statistical",
    description: "ARIMA with seasonal terms P, D, Q at period m",
    strengths: "Explicitly models seasonality; well-studied",
    weaknesses: "Many hyperparameters; slow grid search; single seasonality only",
    hyperparameters: ["p","d","q","P","D","Q","m (seasonal period)"],
    python_class: "SARIMAX(series, order=(p,d,q), seasonal_order=(P,D,Q,m))"
})
CREATE (mARIMAX:Model {
    name: "ARIMAX / SARIMAX",
    family: "Statistical",
    description: "ARIMA/SARIMA with exogenous regressors",
    strengths: "Incorporates external predictors; flexible",
    weaknesses: "Exogenous variables must be known at forecast horizon",
    hyperparameters: ["p","d","q","P","D","Q","m","exog columns"],
    python_class: "SARIMAX(endog, exog=X) from statsmodels"
})
CREATE (mExponential:Model {
    name: "Exponential Smoothing (ETS)",
    family: "Statistical",
    description: "State-space model: Simple (SES), Holt (trend), Holt-Winters (trend+season)",
    strengths: "Robust; fast; handles level/trend/seasonality; good for short-horizon",
    weaknesses: "Limited to additive/multiplicative combinations; no external regressors",
    hyperparameters: ["alpha (level)", "beta (trend)", "gamma (seasonal)", "phi (damping)"],
    python_class: "ExponentialSmoothing(series, trend='add', seasonal='add') from statsmodels"
})
CREATE (mVAR:Model {
    name: "VAR (Vector Autoregression)",
    family: "Statistical",
    description: "Multivariate extension of AR: each variable regresses on lags of all variables",
    strengths: "Captures cross-variable dependencies; Granger causality testing",
    weaknesses: "Number of parameters grows as k² × p; assumes stationarity",
    hyperparameters: ["p (lag order)","k (number of variables)"],
    python_class: "VAR(df).fit(maxlags=p) from statsmodels"
})
CREATE (mProphet:Model {
    name: "Prophet",
    family: "Statistical",
    description: "Additive model from Meta: trend + seasonality + holidays + external regressors",
    strengths: "Handles missing data and holidays well; minimal tuning; business-friendly",
    weaknesses: "Not designed for irregular data or multivariate inputs; can overfit seasonality",
    hyperparameters: ["changepoint_prior_scale","seasonality_mode","n_changepoints"],
    python_class: "Prophet() from prophet"
})
CREATE (mTheta:Model {
    name: "Theta Method",
    family: "Statistical",
    description: "Decomposition-based method that won M3 competition; related to SES with drift",
    strengths: "Strong empirical performance on seasonal data; simple",
    weaknesses: "Additive seasonality assumption; limited configurability",
    hyperparameters: ["theta coefficient"],
    python_class: "ThetaModel(series) from statsmodels"
})

// ─────────────────────────────────────────────────────────────────────────────
// MODEL NODES — MACHINE LEARNING
// ─────────────────────────────────────────────────────────────────────────────
CREATE (mXGB:Model {
    name: "XGBoost",
    family: "Gradient Boosting",
    description: "Gradient boosted trees applied to lag/window features; highly competitive on tabular TS",
    strengths: "Handles non-linearity; built-in feature importance; fast",
    weaknesses: "No native temporal structure awareness; requires careful feature engineering",
    hyperparameters: ["n_estimators","max_depth","learning_rate","subsample","reg_alpha"],
    python_class: "XGBRegressor() / XGBClassifier() from xgboost"
})
CREATE (mLightGBM:Model {
    name: "LightGBM",
    family: "Gradient Boosting",
    description: "Leaf-wise gradient boosting; faster than XGBoost on large datasets",
    strengths: "Very fast; low memory; categorical feature support",
    weaknesses: "Same as XGBoost: no temporal awareness without explicit feature engineering",
    hyperparameters: ["num_leaves","learning_rate","n_estimators","min_child_samples"],
    python_class: "LGBMRegressor() / LGBMClassifier() from lightgbm"
})
CREATE (mRF:Model {
    name: "Random Forest",
    family: "Tree Ensemble",
    description: "Bagged ensemble of decision trees; robust baseline for TS classification/regression",
    strengths: "Low hyperparameter sensitivity; resistant to outliers; feature importance",
    weaknesses: "Cannot extrapolate beyond training range; large memory footprint",
    hyperparameters: ["n_estimators","max_depth","max_features","min_samples_leaf"],
    python_class: "RandomForestRegressor() / RandomForestClassifier() from sklearn"
})
CREATE (mSVR:Model {
    name: "Support Vector Regression / SVC",
    family: "Kernel Method",
    description: "Maximum-margin regression/classification in kernel-mapped feature space",
    strengths: "Effective in high-dimensional spaces; kernel trick for non-linearity",
    weaknesses: "Does not scale to large datasets; requires feature scaling; no uncertainty output",
    hyperparameters: ["C","epsilon","kernel","gamma"],
    python_class: "SVR() / SVC() from sklearn"
})

// ─────────────────────────────────────────────────────────────────────────────
// MODEL NODES — DEEP LEARNING
// ─────────────────────────────────────────────────────────────────────────────
CREATE (mRNN:Model {
    name: "RNN (Vanilla Recurrent Neural Network)",
    family: "Deep Learning",
    description: "Processes sequences step-by-step with shared weights; suffers vanishing gradient",
    strengths: "Sequence-native; variable length input possible",
    weaknesses: "Vanishing gradient makes learning long dependencies hard; superseded by LSTM/GRU",
    hyperparameters: ["hidden_size","num_layers","dropout","learning_rate"],
    python_class: "torch.nn.RNN() or tf.keras.layers.SimpleRNN()"
})
CREATE (mLSTM:Model {
    name: "LSTM (Long Short-Term Memory)",
    family: "Deep Learning",
    description: "RNN with input/forget/output gates enabling long-range dependency learning",
    strengths: "Strong sequential memory; state-of-the-art on many TS tasks until ~2020",
    weaknesses: "Slow to train; sequential computation limits GPU parallelism; many hyperparameters",
    hyperparameters: ["hidden_size","num_layers","dropout","bidirectional","learning_rate","batch_size"],
    python_class: "torch.nn.LSTM() or tf.keras.layers.LSTM()"
})
CREATE (mGRU:Model {
    name: "GRU (Gated Recurrent Unit)",
    family: "Deep Learning",
    description: "Simplified LSTM with reset/update gates; fewer parameters, comparable performance",
    strengths: "Faster than LSTM; fewer parameters; often matches LSTM accuracy",
    weaknesses: "Same sequential computation bottleneck as LSTM",
    hyperparameters: ["hidden_size","num_layers","dropout","learning_rate"],
    python_class: "torch.nn.GRU() or tf.keras.layers.GRU()"
})
CREATE (mCNN1D:Model {
    name: "1D-CNN (Temporal Convolutional Network)",
    family: "Deep Learning",
    description: "Convolutional filters slide over time dimension; captures local patterns",
    strengths: "Highly parallelisable; fast training; good for short-range patterns",
    weaknesses: "Receptive field limited by kernel size/depth; no explicit recurrence",
    hyperparameters: ["filters","kernel_size","dilation_rate","num_layers","dropout"],
    python_class: "torch.nn.Conv1d() or tf.keras.layers.Conv1D()"
})
CREATE (mTCN:Model {
    name: "TCN (Temporal Convolutional Network with Dilated Causal Convolutions)",
    family: "Deep Learning",
    description: "Dilated + causal 1D convolutions for exponentially growing receptive field",
    strengths: "Parallelisable; longer receptive field than standard CNN; no future leakage with causal padding",
    weaknesses: "Fixed receptive field; less memory-efficient for very long sequences",
    hyperparameters: ["num_channels","kernel_size","dilations","dropout"],
    python_class: "pytorch-forecasting TCN or keras-tcn"
})
CREATE (mTransformer:Model {
    name: "Transformer (Self-Attention)",
    family: "Deep Learning",
    description: "Self-attention mechanism captures all-pair temporal dependencies; backbone for modern TS models",
    strengths: "Parallelisable; captures long-range dependencies; adaptable to multivariate",
    weaknesses: "O(n²) attention complexity for long sequences; data-hungry; needs positional encoding",
    hyperparameters: ["d_model","nhead","num_encoder_layers","dim_feedforward","dropout"],
    python_class: "torch.nn.Transformer or Hugging Face time series models"
})
CREATE (mInformer:Model {
    name: "Informer / Autoformer / PatchTST",
    family: "Deep Learning",
    description: "Transformer variants designed for long-sequence time series; ProbSparse / autocorrelation / patch-based attention",
    strengths: "Handle very long input sequences; strong long-term forecasting benchmarks",
    weaknesses: "Complex; heavy compute; may not outperform simple models on short horizons",
    hyperparameters: ["patch_len","stride","d_model","num_heads","e_layers"],
    python_class: "neuralforecast library: PatchTST, Autoformer"
})
CREATE (mNBEATS:Model {
    name: "N-BEATS",
    family: "Deep Learning",
    description: "Deep stack of FC blocks with backward/forward residuals; purely data-driven or interpretable variant",
    strengths: "No exogenous inputs needed; interpretable trend/seasonality decomposition mode",
    weaknesses: "Long training time; requires large datasets; univariate focus",
    hyperparameters: ["stacks","blocks","layers","layer_width","expansion_coefficient_dim"],
    python_class: "neuralforecast library: NBEATS"
})
CREATE (mTFT:Model {
    name: "Temporal Fusion Transformer (TFT)",
    family: "Deep Learning",
    description: "Combines LSTM encoder + self-attention + variable selection networks + quantile output",
    strengths: "Handles static/known-future/unknown covariates; built-in interpretability; quantile intervals",
    weaknesses: "Complex; slow; requires carefully structured dataset with known-future covariates",
    hyperparameters: ["hidden_size","attention_head_size","dropout","hidden_continuous_size","learning_rate"],
    python_class: "pytorch-forecasting TemporalFusionTransformer"
})
CREATE (mDeepAR:Model {
    name: "DeepAR",
    family: "Deep Learning",
    description: "Probabilistic autoregressive LSTM that outputs distribution parameters (mean + std)",
    strengths: "Global model; handles thousands of related series; native uncertainty output",
    weaknesses: "Probabilistic calibration needs validation; slow; requires many series for global training",
    hyperparameters: ["context_length","prediction_length","num_cells","num_layers","dropout_rate"],
    python_class: "gluonts library: DeepAREstimator"
})
CREATE (mTimesNet:Model {
    name: "TimesNet",
    family: "Deep Learning",
    description: "Transforms 1D time series to 2D space using FFT period detection; applies 2D convolutions",
    strengths: "State-of-the-art on 2023 benchmarks for multiple tasks; efficient",
    weaknesses: "Newer; less ecosystem support; period detection can fail on noisy series",
    hyperparameters: ["d_model","d_ff","top_k","num_kernels","e_layers"],
    python_class: "time-series-library (TSlib) TimesNet"
})

// ─────────────────────────────────────────────────────────────────────────────
// MODEL NODES — ANOMALY DETECTION SPECIFIC
// ─────────────────────────────────────────────────────────────────────────────
CREATE (mIsolationForest:Model {
    name: "Isolation Forest",
    family: "Anomaly Detection",
    description: "Unsupervised: isolates anomalies via random feature splits; anomalies have shorter paths",
    strengths: "No need for anomaly labels; scales well; works on windowed features",
    weaknesses: "Does not exploit temporal structure; sensitive to contamination parameter",
    hyperparameters: ["n_estimators","contamination","max_samples","max_features"],
    python_class: "IsolationForest() from sklearn"
})
CREATE (mLOF:Model {
    name: "Local Outlier Factor",
    family: "Anomaly Detection",
    description: "Density-based: compares local reachability density to neighbours",
    strengths: "Detects local anomalies; no distribution assumption",
    weaknesses: "Not efficient on large datasets; no online version; sensitive to k",
    hyperparameters: ["n_neighbors","contamination","metric"],
    python_class: "LocalOutlierFactor() from sklearn"
})
CREATE (mAutoEncoder:Model {
    name: "Autoencoder / LSTM-AE for Anomaly Detection",
    family: "Anomaly Detection",
    description: "Reconstructs normal windows; anomalies have high reconstruction error",
    strengths: "Unsupervised; temporal structure via LSTM encoder; captures complex patterns",
    weaknesses: "Threshold selection for reconstruction error is heuristic; can learn anomalies if too many in training",
    hyperparameters: ["latent_dim","encoder_layers","threshold_percentile","learning_rate"],
    python_class: "Custom PyTorch/Keras LSTM Autoencoder"
})
CREATE (mSRNN:Model {
    name: "Spectral Residual (SR) + CNN",
    family: "Anomaly Detection",
    description: "Microsoft MSRA method: spectral residual of saliency map highlights temporal anomalies",
    strengths: "Unsupervised; fast; interpretable saliency map",
    weaknesses: "Point anomaly focused; less effective on collective anomalies",
    hyperparameters: ["window_size","score_window","mag_window"],
    python_class: "adtk or merlion library"
})

// ─────────────────────────────────────────────────────────────────────────────
// PREDICTION TYPE NODES
// ─────────────────────────────────────────────────────────────────────────────
CREATE (ptForecast:PredictionType {
    name: "Time Series Forecasting",
    description: "Predicting future values of a continuous target variable",
    output: "Continuous future values or distribution",
    examples: "Energy demand, stock price, sales volume, temperature",
    split_strategy: "Temporal train/test split — never random shuffle",
    label_creation: "Shift target by forecast horizon h"
})
CREATE (ptClassification:PredictionType {
    name: "Time Series Classification",
    description: "Assigning a class label to an entire time series window or sequence",
    output: "Discrete class label per window",
    examples: "Fault type identification, activity recognition, ECG diagnosis",
    split_strategy: "Stratified temporal split or subject-wise split",
    label_creation: "Window label from segment annotation or majority vote"
})
CREATE (ptAnomalyDetection:PredictionType {
    name: "Anomaly Detection",
    description: "Identifying unusual observations, sequences, or patterns deviating from normal behaviour",
    output: "Anomaly score, binary flag, or anomaly interval",
    examples: "Industrial fault detection, network intrusion, medical monitoring",
    split_strategy: "Train on normal-only data; test on mixed",
    label_creation: "Often unsupervised; labels used only for evaluation"
})
CREATE (ptRUL:PredictionType {
    name: "Remaining Useful Life (RUL) Prediction",
    description: "Regression predicting cycles/time remaining before failure under current conditions",
    output: "Continuous scalar (cycles or time)",
    examples: "Aircraft engine degradation, battery SOH, bearing wear",
    split_strategy: "Engine/unit-wise split — not temporal — to avoid future leakage",
    label_creation: "RUL = (end_of_life_cycle - current_cycle); often piecewise-linear label"
})
CREATE (ptChangePoint:PredictionType {
    name: "Change Point Detection",
    description: "Locating time points where the statistical properties of the series change",
    output: "Set of change-point timestamps",
    examples: "Process transitions, system regime changes, equipment degradation onset",
    split_strategy: "Offline (batch) or online (sequential) mode",
    label_creation: "Ground truth change points from expert annotation or synthetic injection"
})
CREATE (ptEventDetection:PredictionType {
    name: "Event Detection / Trigger Classification",
    description: "Detecting the onset time of a specific event within a continuous stream",
    output: "Event timestamp or binary trigger per time step",
    examples: "Seizure onset, fault onset, peak detection",
    split_strategy: "Temporal or subject-wise split",
    label_creation: "Binary label per timestamp or window containing event"
})
CREATE (ptPrognostics:PredictionType {
    name: "Prognostics & Health Management (PHM)",
    description: "Combining anomaly detection, fault classification, and RUL in an integrated pipeline",
    output: "Health index + fault class + RUL",
    examples: "Predictive maintenance, avionics health, motor condition monitoring",
    split_strategy: "Unit-wise split (each machine/component is a unit)",
    label_creation: "Multi-output: binary health flag + fault class + RUL scalar"
})
CREATE (ptImputation:PredictionType {
    name: "Time Series Imputation",
    description: "Reconstructing missing values within a historical time series",
    output: "Filled-in values at missing positions",
    examples: "Sensor dropout recovery, medical record completion",
    split_strategy: "Mask known values for evaluation (MCAR holdout)",
    label_creation: "Mask known values artificially as NaN; measure reconstruction error"
})

// ─────────────────────────────────────────────────────────────────────────────
// EVALUATION METRIC NODES
// ─────────────────────────────────────────────────────────────────────────────
CREATE (emMAE:EvalMetric {
    name: "MAE (Mean Absolute Error)",
    formula: "mean(|y - ŷ|)",
    suitable_for: ["Forecasting", "RUL"],
    interpretation: "Average absolute forecast error in original units; robust to outliers",
    pitfall: "Does not penalise large errors more than small ones"
})
CREATE (emRMSE:EvalMetric {
    name: "RMSE (Root Mean Square Error)",
    formula: "sqrt(mean((y - ŷ)²))",
    suitable_for: ["Forecasting", "RUL"],
    interpretation: "Penalises large errors more; in original units",
    pitfall: "Sensitive to outliers; dominated by worst-case errors"
})
CREATE (emMAPE:EvalMetric {
    name: "MAPE (Mean Absolute Percentage Error)",
    formula: "mean(|y - ŷ|/|y|) × 100",
    suitable_for: ["Forecasting"],
    interpretation: "Scale-independent; easy to communicate as %",
    pitfall: "Undefined / unstable when y ≈ 0; asymmetric penalty; biased"
})
CREATE (emSMAPE:EvalMetric {
    name: "sMAPE (Symmetric MAPE)",
    formula: "mean(2|y - ŷ|/(|y|+|ŷ|)) × 100",
    suitable_for: ["Forecasting"],
    interpretation: "More symmetric than MAPE; bounded [0%, 200%]",
    pitfall: "Still undefined when both y and ŷ = 0; not truly symmetric in all cases"
})
CREATE (emMASE:EvalMetric {
    name: "MASE (Mean Absolute Scaled Error)",
    formula: "MAE / MAE_naive_seasonal",
    suitable_for: ["Forecasting"],
    interpretation: "Scale-free; MASE < 1 means better than seasonal naïve; robust",
    pitfall: "Requires defining naive baseline; complex to explain to business stakeholders"
})
CREATE (emR2:EvalMetric {
    name: "R² (Coefficient of Determination)",
    formula: "1 - SS_res / SS_tot",
    suitable_for: ["Forecasting", "RUL"],
    interpretation: "Fraction of variance explained; 1.0 is perfect; can be negative",
    pitfall: "Can be misleading on non-stationary series; does not penalise bias"
})
CREATE (emAccuracy:EvalMetric {
    name: "Accuracy",
    formula: "correct_predictions / total_predictions",
    suitable_for: ["Classification"],
    interpretation: "Simple; intuitive for balanced classes",
    pitfall: "Misleading for imbalanced classes (95% normal = 95% accuracy by predicting all normal)"
})
CREATE (emF1:EvalMetric {
    name: "F1-Score",
    formula: "2 × (Precision × Recall) / (Precision + Recall)",
    suitable_for: ["Classification", "Anomaly Detection"],
    interpretation: "Harmonic mean of precision and recall; balanced view of binary performance",
    pitfall: "Requires threshold selection; macro/micro/weighted variants give different views"
})
CREATE (emAUROC:EvalMetric {
    name: "AUROC (Area Under ROC Curve)",
    formula: "Area under TPR vs FPR curve",
    suitable_for: ["Classification", "Anomaly Detection"],
    interpretation: "Threshold-independent; probability that model ranks a positive above a negative",
    pitfall: "Optimistic when classes are highly imbalanced; prefer AUPRC in that case"
})
CREATE (emAUPRC:EvalMetric {
    name: "AUPRC (Area Under Precision-Recall Curve)",
    formula: "Area under Precision vs Recall curve",
    suitable_for: ["Anomaly Detection", "Classification with imbalance"],
    interpretation: "More informative than AUROC on imbalanced datasets; emphasises minority class",
    pitfall: "Depends on class ratio; not comparable across datasets with different imbalance"
})
CREATE (emCRPS:EvalMetric {
    name: "CRPS (Continuous Ranked Probability Score)",
    formula: "E[|F - 1(y ≤ x)|²] integrated over x",
    suitable_for: ["Probabilistic Forecasting"],
    interpretation: "Evaluates full predictive distribution; lower is better; proper scoring rule",
    pitfall: "Complex to compute; requires distributional forecast output"
})
CREATE (emPinball:EvalMetric {
    name: "Pinball Loss (Quantile Loss)",
    formula: "max(q(y-ŷ), (q-1)(y-ŷ)) averaged over quantiles",
    suitable_for: ["Probabilistic Forecasting", "TFT"],
    interpretation: "Proper scoring rule for quantile forecasts; asymmetric penalty per quantile",
    pitfall: "Requires specifying which quantiles to evaluate"
})
CREATE (emNAD:EvalMetric {
    name: "NAD (Normalised Anomaly Detection Score)",
    formula: "Precision@k or Time-to-Detect (TTD)",
    suitable_for: ["Anomaly Detection"],
    interpretation: "Time-to-detect evaluates operational usefulness of early warning",
    pitfall: "Ground truth labelling for anomalies is often noisy or ambiguous"
})
CREATE (emRULScore:EvalMetric {
    name: "NASA RUL Scoring Function",
    formula: "sum(exp(error/13)-1 for early; exp(-error/10)-1 for late)",
    suitable_for: ["RUL Prediction"],
    interpretation: "Asymmetric: penalises late predictions more than early (conservative is safer)",
    pitfall: "Task-specific; not generalisable to other regression problems"
})
CREATE (emRULRMSE:EvalMetric {
    name: "RMSE on RUL",
    formula: "sqrt(mean((RUL_true - RUL_pred)²))",
    suitable_for: ["RUL Prediction"],
    interpretation: "Standard; lower is better; reported in cycles for CMAPSS datasets",
    pitfall: "Symmetric penalty may not reflect operational cost asymmetry"
})
CREATE (emMCC:EvalMetric {
    name: "MCC (Matthews Correlation Coefficient)",
    formula: "(TP×TN - FP×FN)/sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN))",
    suitable_for: ["Classification", "Anomaly Detection"],
    interpretation: "Balanced metric even with severe class imbalance; range [-1, 1]",
    pitfall: "Less intuitive than F1 for non-technical stakeholders"
})

// ─────────────────────────────────────────────────────────────────────────────
// LIBRARY NODES
// ─────────────────────────────────────────────────────────────────────────────
CREATE (libPandas:Library    { name: "pandas",               install: "pip install pandas",               docs: "https://pandas.pydata.org",                 use: "Data loading, time indexing, resampling, rolling ops" })
CREATE (libNumpy:Library     { name: "numpy",                install: "pip install numpy",                docs: "https://numpy.org",                         use: "Array ops, FFT, windowing" })
CREATE (libStatsmodels:Library{name: "statsmodels",          install: "pip install statsmodels",          docs: "https://www.statsmodels.org",               use: "ARIMA, SARIMA, ETS, ACF/PACF, ADF/KPSS, VAR" })
CREATE (libSklearn:Library   { name: "scikit-learn",         install: "pip install scikit-learn",         docs: "https://scikit-learn.org",                  use: "Preprocessing scalers, RF, SVM, IsolationForest, TimeSeriesSplit" })
CREATE (libPytorch:Library   { name: "PyTorch",              install: "pip install torch",                docs: "https://pytorch.org",                       use: "LSTM, GRU, CNN, Transformer implementation" })
CREATE (libTF:Library        { name: "TensorFlow/Keras",     install: "pip install tensorflow",           docs: "https://tensorflow.org",                    use: "LSTM, GRU, CNN, high-level model building" })
CREATE (libXGB:Library       { name: "XGBoost",              install: "pip install xgboost",              docs: "https://xgboost.readthedocs.io",            use: "Gradient boosted trees for tabular TS" })
CREATE (libLGBM:Library      { name: "LightGBM",             install: "pip install lightgbm",             docs: "https://lightgbm.readthedocs.io",           use: "Fast gradient boosted trees" })
CREATE (libNixtla:Library    { name: "neuralforecast",       install: "pip install neuralforecast",       docs: "https://nixtla.github.io/neuralforecast",   use: "NBEATS, TFT, PatchTST, Autoformer" })
CREATE (libGluon:Library     { name: "gluonts",              install: "pip install gluonts",              docs: "https://ts.gluon.ai",                       use: "DeepAR, WaveNet, Transformer; probabilistic forecasting" })
CREATE (libProphLib:Library  { name: "prophet",              install: "pip install prophet",              docs: "https://facebook.github.io/prophet",        use: "Prophet forecasting with holidays" })
CREATE (libPykalman:Library  { name: "pykalman",             install: "pip install pykalman",             docs: "https://pykalman.github.io",                use: "Kalman filter/smoother imputation" })
CREATE (libTsaug:Library     { name: "tsaug",                install: "pip install tsaug",                docs: "https://tsaug.readthedocs.io",              use: "Time series augmentation: jitter, warp, crop, drift" })
CREATE (libTsfresh:Library   { name: "tsfresh",              install: "pip install tsfresh",              docs: "https://tsfresh.readthedocs.io",            use: "Automated time series feature extraction (700+ features)" })
CREATE (libPywt:Library      { name: "PyWavelets",           install: "pip install PyWavelets",           docs: "https://pywavelets.readthedocs.io",         use: "Wavelet transform for time-frequency features" })
CREATE (libRuptures:Library  { name: "ruptures",             install: "pip install ruptures",             docs: "https://centre-borelli.github.io/ruptures-docs", use: "Change point detection (PELT, BINSEG, Window)" })
CREATE (libMerlion:Library   { name: "merlion",              install: "pip install salesforce-merlion",   docs: "https://opensource.salesforce.com/Merlion", use: "Anomaly detection & forecasting suite" })
CREATE (libAdtk:Library      { name: "adtk",                 install: "pip install adtk",                 docs: "https://adtk.readthedocs.io",               use: "Rule-based and statistical anomaly detection" })
CREATE (libOptuna:Library    { name: "optuna",               install: "pip install optuna",               docs: "https://optuna.org",                        use: "Hyperparameter optimisation (Bayesian)" })
CREATE (libMlflow:Library    { name: "MLflow",               install: "pip install mlflow",               docs: "https://mlflow.org",                        use: "Experiment tracking, model registry, deployment" })
CREATE (libEvidentlyLib:Library{name: "Evidently AI",        install: "pip install evidently",            docs: "https://evidentlyai.com",                   use: "Data / prediction drift monitoring" })
CREATE (libImbalanced:Library { name: "imbalanced-learn",   install: "pip install imbalanced-learn",     docs: "https://imbalanced-learn.org",              use: "SMOTE, ADASYN, RandomUnderSampler for class imbalance" })
CREATE (libDtaidist:Library  { name: "dtaidistance",         install: "pip install dtaidistance",         docs: "https://dtaidistance.readthedocs.io",       use: "DTW distance, barycenter averaging" })
CREATE (libMissingno:Library { name: "missingno",            install: "pip install missingno",            docs: "https://github.com/ResidentMario/missingno", use: "Missing value pattern visualisation" })
CREATE (libScipy:Library     { name: "scipy",                install: "pip install scipy",                docs: "https://scipy.org",                         use: "Signal processing: FFT, filters, interpolation, stats" })
CREATE (libMatplotlib:Library { name: "matplotlib",          install: "pip install matplotlib",           docs: "https://matplotlib.org",                    use: "Low-level plotting" })
CREATE (libSeaborn:Library   { name: "seaborn",              install: "pip install seaborn",              docs: "https://seaborn.pydata.org",                use: "Statistical visualisation" })

// ─────────────────────────────────────────────────────────────────────────────
// USE CASE NODES
// ─────────────────────────────────────────────────────────────────────────────
CREATE (ucPredMaint:UseCase  { name: "Predictive Maintenance",     domain: "Industrial/IoT",    description: "Detect faults early and estimate RUL to prevent unplanned downtime", benchmark_dataset: "CMAPSS / PRONOSTIA" })
CREATE (ucEnergyForecast:UseCase{ name: "Energy Demand Forecasting", domain: "Energy/Utilities", description: "Forecast power load, consumption, or generation at various horizons", benchmark_dataset: "ETT / GEFCom" })
CREATE (ucFinancial:UseCase  { name: "Financial Time Series",       domain: "Finance",           description: "Price prediction, volatility forecasting, and regime detection", benchmark_dataset: "M4 Competition / S&P 500" })
CREATE (ucIoT:UseCase        { name: "IoT Sensor Monitoring",       domain: "IoT/Manufacturing", description: "Continuous sensor stream monitoring, anomaly flagging, imputation", benchmark_dataset: "SKAB / MIT-BIH" })
CREATE (ucHealthcare:UseCase { name: "Healthcare / Clinical TS",    domain: "Healthcare",        description: "ICU vital sign monitoring, sepsis prediction, ECG classification", benchmark_dataset: "PhysioNet / MIMIC-III" })
CREATE (ucSales:UseCase      { name: "Retail Sales Forecasting",    domain: "Retail",            description: "SKU-level demand forecasting accounting for promotions and seasonality", benchmark_dataset: "M5 Competition / Rossman" })
CREATE (ucWeather:UseCase    { name: "Weather / Climate Forecasting",domain: "Meteorology",      description: "Short-term and seasonal weather forecasting", benchmark_dataset: "Weather dataset / ERA5" })
CREATE (ucNetworkAD:UseCase  { name: "Network Anomaly Detection",   domain: "Cybersecurity",     description: "Detect intrusions or unusual traffic patterns in network flow data", benchmark_dataset: "KDD Cup 1999 / CICIDS" })

// ─────────────────────────────────────────────────────────────────────────────
// CONSIDERATION / BEST PRACTICE NODES (Do's)
// ─────────────────────────────────────────────────────────────────────────────
CREATE (bpTemporalSplit:BestPractice {
    name: "Always Use Temporal Train/Test Split",
    type: "DO",
    description: "Train data must chronologically precede test data. Never use random shuffle for TS.",
    consequence_if_ignored: "Data leakage — future information in training set causes inflated metrics",
    applies_to: "All prediction types"
})
CREATE (bpFitOnTrain:BestPractice {
    name: "Fit Scalers/Encoders Only on Training Data",
    type: "DO",
    description: "All preprocessing transformers (scalers, encoders, imputers) must be fit on training fold only and applied to val/test.",
    consequence_if_ignored: "Look-ahead bias from test statistics leaks into training",
    applies_to: "Preprocessing"
})
CREATE (bpBaselineFirst:BestPractice {
    name: "Establish a Naïve Baseline First",
    type: "DO",
    description: "Before any ML: compute naïve seasonal (repeat last season) and persistence (repeat last value) baselines.",
    consequence_if_ignored: "Complex models that underperform a naïve baseline are worthless and undetected",
    applies_to: "Modelling"
})
CREATE (bpResidualAnalysis:BestPractice {
    name: "Perform Residual Diagnostics",
    type: "DO",
    description: "After model fitting: check residual ACF (should be white noise), histogram (should be normal), and residuals vs time (no pattern).",
    consequence_if_ignored: "Remaining autocorrelation means model is misspecified",
    applies_to: "Evaluation"
})
CREATE (bpUncertainty:BestPractice {
    name: "Report Prediction Intervals, Not Just Point Forecasts",
    type: "DO",
    description: "Communicate uncertainty via confidence/prediction intervals or quantile forecasts.",
    consequence_if_ignored: "Overconfident point forecasts mislead business decisions",
    applies_to: "Forecasting, RUL"
})
CREATE (bpCrossValTS:BestPractice {
    name: "Use Time Series Cross-Validation",
    type: "DO",
    description: "Walk-forward or sliding-window CV gives honest estimate of production performance across multiple folds.",
    consequence_if_ignored: "Single train/val split gives high-variance performance estimate",
    applies_to: "Model Selection"
})
CREATE (bpEarlyStop:BestPractice {
    name: "Use Early Stopping for DL Models",
    type: "DO",
    description: "Monitor validation loss and stop training when it stops improving to prevent overfitting.",
    consequence_if_ignored: "DL models overfit temporal training patterns",
    applies_to: "Deep Learning Modelling"
})
CREATE (bpHyperparamOpt:BestPractice {
    name: "Use Bayesian Hyperparameter Optimisation",
    type: "DO",
    description: "Use Optuna or similar to tune hyperparameters efficiently vs manual grid search.",
    consequence_if_ignored: "Suboptimal models or wasted compute on grid search",
    applies_to: "Modelling"
})
CREATE (bpDomainKnowledge:BestPractice {
    name: "Incorporate Domain Knowledge in Feature Engineering",
    type: "DO",
    description: "Physics-based features (wear indicators, frequency bands) outperform purely data-driven features in industrial applications.",
    consequence_if_ignored: "Black-box features miss domain-relevant signal",
    applies_to: "Feature Engineering"
})
CREATE (bpMonitor:BestPractice {
    name: "Monitor for Data and Concept Drift in Production",
    type: "DO",
    description: "Track input feature distributions and prediction distributions over time. Retrain when drift is detected.",
    consequence_if_ignored: "Silent model degradation in production",
    applies_to: "Deployment"
})
CREATE (bpVersionData:BestPractice {
    name: "Version Your Datasets and Preprocessing Pipelines",
    type: "DO",
    description: "Use DVC or MLflow to track which data version and preprocessing code produced each model.",
    consequence_if_ignored: "Irreproducible results; cannot roll back or audit predictions",
    applies_to: "MLOps"
})
CREATE (bpLabelQuality:BestPractice {
    name: "Validate Label Quality Before Training",
    type: "DO",
    description: "Inspect fault labels, RUL labels, or anomaly labels for timing errors, missing events, or systematic biases.",
    consequence_if_ignored: "Garbage labels produce garbage models regardless of algorithm sophistication",
    applies_to: "Data Quality"
})

// ─────────────────────────────────────────────────────────────────────────────
// ANTI-PATTERN NODES (Don'ts)
// ─────────────────────────────────────────────────────────────────────────────
CREATE (apRandomSplit:AntiPattern {
    name: "Random Train/Test Split on Time Series",
    type: "DONT",
    description: "Shuffling observations ignores temporal dependency and causes data leakage.",
    correct_approach: "Always split by time: train on past, test on future",
    consequence: "Unrealistically high test metrics that collapse in production"
})
CREATE (apFitOnAll:AntiPattern {
    name: "Fit Scaler on Full Dataset Before Split",
    type: "DONT",
    description: "Scaling using statistics from test data leaks test information into the model.",
    correct_approach: "Split first; fit scaler on train only; transform train+test",
    consequence: "Data leakage; inflated validation metrics"
})
CREATE (apIgnoreStationarity:AntiPattern {
    name: "Running ARIMA on Non-Stationary Series Without Differencing",
    type: "DONT",
    description: "ARIMA on a unit-root series produces spurious correlations and incorrect forecasts.",
    correct_approach: "Test with ADF/KPSS; difference until stationary; verify with residual ACF",
    consequence: "Misleading coefficients; non-convergence; diverging forecasts"
})
CREATE (apTestLeakInCV:AntiPattern {
    name: "Using Future Data in Validation Fold Features",
    type: "DONT",
    description: "Rolling features computed across train/test boundary use future test information.",
    correct_approach: "Compute all rolling/lag features strictly within the training window",
    consequence: "Optimistic CV scores that cannot be replicated in production"
})
CREATE (apSMOTEBeforeSplit:AntiPattern {
    name: "Applying SMOTE Before Train/Test Split",
    type: "DONT",
    description: "Synthetic minority samples generated from both train and test data contaminate evaluation.",
    correct_approach: "Apply SMOTE only on training fold after splitting",
    consequence: "Test set contaminated with synthetic neighbours; inflated minority class recall"
})
CREATE (apOverSmooth:AntiPattern {
    name: "Aggressive Smoothing Before Anomaly Detection",
    type: "DONT",
    description: "Heavy smoothing removes the sharp transients that characterise point anomalies.",
    correct_approach: "Use mild smoothing for trend extraction; evaluate on raw signal",
    consequence: "Missed anomalies; reduced sensitivity"
})
CREATE (apIgnoreLeakage:AntiPattern {
    name: "Including Future-Derived Features",
    type: "DONT",
    description: "Features such as 'remaining cycles until failure' or 'time to end of dataset' embed label information.",
    correct_approach: "Ensure all features are computable from data available at prediction time",
    consequence: "Model learns the label directly rather than degradation patterns"
})
CREATE (apOneMetric:AntiPattern {
    name: "Evaluating Only on Accuracy or RMSE",
    type: "DONT",
    description: "Single metric misses important aspects: accuracy ignores imbalance; RMSE is dominated by outliers.",
    correct_approach: "Report multiple complementary metrics appropriate to the task",
    consequence: "Models that optimise one metric while performing poorly on the operational goal"
})
CREATE (apNoBaseline:AntiPattern {
    name: "Skipping the Naïve Baseline",
    type: "DONT",
    description: "Reporting model performance without a naïve or persistence baseline makes it impossible to gauge value-add.",
    correct_approach: "Always compute seasonal naïve and persistence baselines; report relative improvement",
    consequence: "Models that underperform trivial baselines go undetected"
})
CREATE (apLookAheadWindow:AntiPattern {
    name: "Look-Ahead Bias in Window Features",
    type: "DONT",
    description: "Centred rolling windows use future values from within the window.",
    correct_approach: "Use trailing-only (right-aligned) windows: window ends at current timestep",
    consequence: "Model has access to future data it cannot have in production"
});

// ═══════════════════════════════════════════════════════════════════════════════
// TIME SERIES KG — PART 2: RELATIONSHIPS
// Run AFTER Part 1 (nodes must exist)
// ═══════════════════════════════════════════════════════════════════════════════

// ─────────────────────────────────────────────────────────────────────────────
// PIPELINE STAGE → LEADS_TO (ordered pipeline flow)
// ─────────────────────────────────────────────────────────────────────────────
MATCH (a:PipelineStage {name:"Data Ingestion"}),           (b:PipelineStage {name:"Exploratory Data Analysis"})  CREATE (a)-[:LEADS_TO {order:1}]->(b);
MATCH (a:PipelineStage {name:"Exploratory Data Analysis"}),(b:PipelineStage {name:"Preprocessing"})               CREATE (a)-[:LEADS_TO {order:2}]->(b);
MATCH (a:PipelineStage {name:"Preprocessing"}),            (b:PipelineStage {name:"Sampling & Windowing"})        CREATE (a)-[:LEADS_TO {order:3}]->(b);
MATCH (a:PipelineStage {name:"Sampling & Windowing"}),     (b:PipelineStage {name:"Data Augmentation"})           CREATE (a)-[:LEADS_TO {order:4}]->(b);
MATCH (a:PipelineStage {name:"Data Augmentation"}),        (b:PipelineStage {name:"Feature Engineering"})         CREATE (a)-[:LEADS_TO {order:5}]->(b);
MATCH (a:PipelineStage {name:"Feature Engineering"}),      (b:PipelineStage {name:"Modelling"})                   CREATE (a)-[:LEADS_TO {order:6}]->(b);
MATCH (a:PipelineStage {name:"Modelling"}),                (b:PipelineStage {name:"Evaluation"})                  CREATE (a)-[:LEADS_TO {order:7}]->(b);
MATCH (a:PipelineStage {name:"Evaluation"}),               (b:PipelineStage {name:"Deployment & Monitoring"})     CREATE (a)-[:LEADS_TO {order:8}]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// PIPELINE STAGE → USES TECHNIQUE
// ─────────────────────────────────────────────────────────────────────────────
// EDA Techniques
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Time Series Line Plot"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"ACF Plot"})                        CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"PACF Plot"})                       CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Augmented Dickey-Fuller Test"})    CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"KPSS Test"})                       CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Seasonal Decomposition"})          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"STL Decomposition"})               CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Rolling Mean & Std Dev"})          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Lag Plot"})                        CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"IQR Outlier Detection"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Z-Score Outlier Detection"})       CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Missing Value Pattern Analysis"})  CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Histogram / KDE Plot"})            CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Cross-Correlation Plot"})          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Granger Causality Test"})          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Exploratory Data Analysis"}), (t:Technique {name:"Change Point Detection"})          CREATE (s)-[:USES]->(t);

// Preprocessing Techniques
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Resampling / Frequency Alignment"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Differencing"})                               CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Log Transform"})                              CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Box-Cox Transform"})                          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Min-Max Scaling"})                            CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Z-Score Standardisation"})                    CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Robust Scaling"})                             CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Moving Average Smoothing"})                   CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Exponentially Weighted Moving Average"})      CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Savitzky-Golay Filter"})                      CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Outlier Clipping / Winsorisation"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Detrending"})                                 CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Deseasoning"})                                CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Time Index Alignment"})                       CREATE (s)-[:USES]->(t);
// Imputation techniques belong to preprocessing
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Forward Fill (Last Observation Carried Forward)"}) CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Backward Fill"})                              CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Linear Interpolation"})                       CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Spline Interpolation"})                       CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Kalman Filter Imputation"})                   CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"MICE (Multiple Imputation by Chained Equations)"}) CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"Seasonal Mean Imputation"})                   CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Preprocessing"}), (t:Technique {name:"GAN-Based Imputation (GAIN / GRUI)"})         CREATE (s)-[:USES]->(t);

// Sampling Techniques
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Sliding Window"})                      CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Tumbling (Non-Overlapping) Window"})   CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Expanding Window (Walk-Forward) CV"})  CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Sliding Window Cross-Validation"})     CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Stratified Temporal Split"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Temporal Downsampling"})               CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Sampling & Windowing"}), (t:Technique {name:"Temporal Upsampling"})                 CREATE (s)-[:USES]->(t);

// Augmentation Techniques
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Jittering (Gaussian Noise Addition)"})    CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Magnitude Scaling"})                      CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Time Warping"})                           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Window Slicing"})                         CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"DTW Barycentric Averaging"})              CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"GAN-Based Augmentation (TimeGAN / TGAN)"}) CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Frequency Domain Augmentation"})          CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"Time Series Mixup"})                      CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Data Augmentation"}), (t:Technique {name:"SMOTE for Time Series"})                  CREATE (s)-[:USES]->(t);

// Feature Engineering Techniques
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Lag Features"})                         CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Rolling Statistical Features"})         CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"EWM Features"})                         CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"FFT / Spectral Features"})              CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Wavelet Transform Features"})           CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Calendar / Temporal Features"})         CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"tsfresh Automated Feature Extraction"}) CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Cumulative / Integrated Features"})     CREATE (s)-[:USES]->(t);
MATCH (s:PipelineStage {name:"Feature Engineering"}), (t:Technique {name:"Target Encoding of Categorical Covariates"}) CREATE (s)-[:USES]->(t);

// ─────────────────────────────────────────────────────────────────────────────
// PIPELINE STAGE → USES MODEL
// ─────────────────────────────────────────────────────────────────────────────
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"AR (Autoregressive)"})               CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"MA (Moving Average)"})               CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"ARIMA"})                             CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"SARIMA"})                            CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"ARIMAX / SARIMAX"})                  CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Exponential Smoothing (ETS)"})       CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"VAR (Vector Autoregression)"})       CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Prophet"})                           CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Theta Method"})                      CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"XGBoost"})                           CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"LightGBM"})                          CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Random Forest"})                     CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Support Vector Regression / SVC"})   CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"RNN (Vanilla Recurrent Neural Network)"}) CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"LSTM (Long Short-Term Memory)"})     CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"GRU (Gated Recurrent Unit)"})        CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"1D-CNN (Temporal Convolutional Network)"}) CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}) CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Transformer (Self-Attention)"})      CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Informer / Autoformer / PatchTST"})  CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"N-BEATS"})                           CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Temporal Fusion Transformer (TFT)"}) CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"DeepAR"})                            CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"TimesNet"})                          CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Isolation Forest"})                  CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Local Outlier Factor"})              CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}) CREATE (s)-[:USES]->(m);
MATCH (s:PipelineStage {name:"Modelling"}), (m:Model {name:"Spectral Residual (SR) + CNN"})      CREATE (s)-[:USES]->(m);

// ─────────────────────────────────────────────────────────────────────────────
// MODEL → SUITABLE_FOR Prediction Type
// ─────────────────────────────────────────────────────────────────────────────
MATCH (m:Model {name:"ARIMA"}),                              (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"SARIMA"}),                             (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"ARIMAX / SARIMAX"}),                   (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Exponential Smoothing (ETS)"}),        (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Prophet"}),                            (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Theta Method"}),                       (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"VAR (Vector Autoregression)"}),        (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"N-BEATS"}),                            (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"DeepAR"}),                             (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Temporal Fusion Transformer (TFT)"}),  (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Informer / Autoformer / PatchTST"}),   (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"TimesNet"}),                           (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),      (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),         (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(p:PredictionType {name:"Time Series Forecasting"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"XGBoost"}),                            (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"LightGBM"}),                           (p:PredictionType {name:"Time Series Forecasting"})   CREATE (m)-[:SUITABLE_FOR]->(p);

MATCH (m:Model {name:"XGBoost"}),                            (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"LightGBM"}),                           (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Random Forest"}),                      (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Support Vector Regression / SVC"}),    (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),      (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),         (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"1D-CNN (Temporal Convolutional Network)"}),(p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Transformer (Self-Attention)"}),        (p:PredictionType {name:"Time Series Classification"}) CREATE (m)-[:SUITABLE_FOR]->(p);

MATCH (m:Model {name:"Isolation Forest"}),                   (p:PredictionType {name:"Anomaly Detection"})         CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Local Outlier Factor"}),               (p:PredictionType {name:"Anomaly Detection"})         CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}),(p:PredictionType {name:"Anomaly Detection"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Spectral Residual (SR) + CNN"}),       (p:PredictionType {name:"Anomaly Detection"})         CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),      (p:PredictionType {name:"Anomaly Detection"})         CREATE (m)-[:SUITABLE_FOR]->(p);

MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),      (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),         (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"XGBoost"}),                            (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Random Forest"}),                      (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Temporal Fusion Transformer (TFT)"}),  (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"1D-CNN (Temporal Convolutional Network)"}),(p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (m)-[:SUITABLE_FOR]->(p);

MATCH (m:Model {name:"Spectral Residual (SR) + CNN"}),       (p:PredictionType {name:"Change Point Detection"})    CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Isolation Forest"}),                   (p:PredictionType {name:"Change Point Detection"})    CREATE (m)-[:SUITABLE_FOR]->(p);

MATCH (m:Model {name:"DeepAR"}),                             (p:PredictionType {name:"Time Series Imputation"})    CREATE (m)-[:SUITABLE_FOR]->(p);
MATCH (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}),(p:PredictionType {name:"Time Series Imputation"}) CREATE (m)-[:SUITABLE_FOR]->(p);

// ─────────────────────────────────────────────────────────────────────────────
// PREDICTION TYPE → EVALUATED_BY Metric
// ─────────────────────────────────────────────────────────────────────────────
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"MAE (Mean Absolute Error)"})          CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"RMSE (Root Mean Square Error)"})       CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"MAPE (Mean Absolute Percentage Error)"}) CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"sMAPE (Symmetric MAPE)"})             CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"MASE (Mean Absolute Scaled Error)"})   CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"CRPS (Continuous Ranked Probability Score)"}) CREATE (p)-[:EVALUATED_BY {primary:false, note:"for probabilistic forecasts"}]->(m);
MATCH (p:PredictionType {name:"Time Series Forecasting"}),             (m:EvalMetric {name:"Pinball Loss (Quantile Loss)"})        CREATE (p)-[:EVALUATED_BY {primary:false, note:"for quantile forecasts"}]->(m);

MATCH (p:PredictionType {name:"Time Series Classification"}),          (m:EvalMetric {name:"Accuracy"})                           CREATE (p)-[:EVALUATED_BY {primary:false, note:"only for balanced classes"}]->(m);
MATCH (p:PredictionType {name:"Time Series Classification"}),          (m:EvalMetric {name:"F1-Score"})                           CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Classification"}),          (m:EvalMetric {name:"AUROC (Area Under ROC Curve)"})        CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Classification"}),          (m:EvalMetric {name:"MCC (Matthews Correlation Coefficient)"}) CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);

MATCH (p:PredictionType {name:"Anomaly Detection"}),                   (m:EvalMetric {name:"F1-Score"})                           CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Anomaly Detection"}),                   (m:EvalMetric {name:"AUPRC (Area Under Precision-Recall Curve)"}) CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Anomaly Detection"}),                   (m:EvalMetric {name:"AUROC (Area Under ROC Curve)"})        CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);
MATCH (p:PredictionType {name:"Anomaly Detection"}),                   (m:EvalMetric {name:"NAD (Normalised Anomaly Detection Score)"}) CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);
MATCH (p:PredictionType {name:"Anomaly Detection"}),                   (m:EvalMetric {name:"MCC (Matthews Correlation Coefficient)"}) CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);

MATCH (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}),(m:EvalMetric {name:"RMSE on RUL"})                     CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}),(m:EvalMetric {name:"NASA RUL Scoring Function"})        CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}),(m:EvalMetric {name:"MAE (Mean Absolute Error)"})        CREATE (p)-[:EVALUATED_BY {primary:false}]->(m);

MATCH (p:PredictionType {name:"Time Series Imputation"}),              (m:EvalMetric {name:"MAE (Mean Absolute Error)"})          CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);
MATCH (p:PredictionType {name:"Time Series Imputation"}),              (m:EvalMetric {name:"RMSE (Root Mean Square Error)"})       CREATE (p)-[:EVALUATED_BY {primary:true}]->(m);

// ─────────────────────────────────────────────────────────────────────────────
// CONCEPT → ADDRESSED_BY Technique
// ─────────────────────────────────────────────────────────────────────────────
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"Augmented Dickey-Fuller Test"})         CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"KPSS Test"})                            CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"Rolling Mean & Std Dev"})               CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"Differencing"})                         CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"Detrending"})                           CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Stationarity"}), (t:Technique {name:"Deseasoning"})                          CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Trend"}), (t:Technique {name:"Seasonal Decomposition"})                      CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Trend"}), (t:Technique {name:"STL Decomposition"})                           CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Trend"}), (t:Technique {name:"Detrending"})                                  CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Trend"}), (t:Technique {name:"Differencing"})                                CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Seasonality"}), (t:Technique {name:"Seasonal Decomposition"})                CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Seasonality"}), (t:Technique {name:"STL Decomposition"})                     CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Seasonality"}), (t:Technique {name:"Deseasoning"})                           CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Seasonality"}), (t:Technique {name:"ACF Plot"})                              CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Autocorrelation"}), (t:Technique {name:"ACF Plot"})                          CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Autocorrelation"}), (t:Technique {name:"Lag Plot"})                          CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Partial Autocorrelation"}), (t:Technique {name:"PACF Plot"})                 CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Data Leakage"}), (t:Technique {name:"Expanding Window (Walk-Forward) CV"})   CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Data Leakage"}), (t:Technique {name:"Sliding Window Cross-Validation"})      CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Irregular Sampling"}), (t:Technique {name:"Resampling / Frequency Alignment"})  CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Irregular Sampling"}), (t:Technique {name:"Time Index Alignment"})              CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Irregular Sampling"}), (t:Technique {name:"Linear Interpolation"})              CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Irregular Sampling"}), (t:Technique {name:"Spline Interpolation"})              CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Normalization / Scaling"}), (t:Technique {name:"Min-Max Scaling"})           CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Normalization / Scaling"}), (t:Technique {name:"Z-Score Standardisation"})   CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Normalization / Scaling"}), (t:Technique {name:"Robust Scaling"})            CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Change Point"}), (t:Technique {name:"Change Point Detection"})               CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Granger Causality"}), (t:Technique {name:"Granger Causality Test"})          CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Class Imbalance in Time Series"}), (t:Technique {name:"SMOTE for Time Series"}) CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Class Imbalance in Time Series"}), (t:Technique {name:"Jittering (Gaussian Noise Addition)"}) CREATE (t)-[:ADDRESSES]->(c);
MATCH (c:Concept {name:"Class Imbalance in Time Series"}), (t:Technique {name:"GAN-Based Augmentation (TimeGAN / TGAN)"}) CREATE (t)-[:ADDRESSES]->(c);

MATCH (c:Concept {name:"Concept Drift"}), (t:Technique {name:"Change Point Detection"})              CREATE (t)-[:ADDRESSES]->(c);

// ─────────────────────────────────────────────────────────────────────────────
// TECHNIQUE → IMPLEMENTED_IN Library
// ─────────────────────────────────────────────────────────────────────────────
MATCH (t:Technique {name:"ACF Plot"}),                            (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"PACF Plot"}),                           (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Augmented Dickey-Fuller Test"}),        (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"KPSS Test"}),                           (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Seasonal Decomposition"}),              (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"STL Decomposition"}),                   (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Granger Causality Test"}),              (l:Library {name:"statsmodels"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Time Series Line Plot"}),               (l:Library {name:"matplotlib"})  CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Rolling Mean & Std Dev"}),              (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Lag Plot"}),                            (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"IQR Outlier Detection"}),               (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Z-Score Outlier Detection"}),           (l:Library {name:"scipy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Missing Value Pattern Analysis"}),      (l:Library {name:"missingno"})   CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Histogram / KDE Plot"}),                (l:Library {name:"seaborn"})     CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Resampling / Frequency Alignment"}),    (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Differencing"}),                        (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Log Transform"}),                       (l:Library {name:"numpy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Box-Cox Transform"}),                   (l:Library {name:"scipy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Min-Max Scaling"}),                     (l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Z-Score Standardisation"}),             (l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Robust Scaling"}),                      (l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Moving Average Smoothing"}),            (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Exponentially Weighted Moving Average"}),(l:Library {name:"pandas"})     CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Savitzky-Golay Filter"}),               (l:Library {name:"scipy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Outlier Clipping / Winsorisation"}),    (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Detrending"}),                          (l:Library {name:"scipy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Time Index Alignment"}),                (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Forward Fill (Last Observation Carried Forward)"}),(l:Library {name:"pandas"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Backward Fill"}),                       (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Linear Interpolation"}),                (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Spline Interpolation"}),                (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Kalman Filter Imputation"}),            (l:Library {name:"pykalman"})    CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"MICE (Multiple Imputation by Chained Equations)"}),(l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"GAN-Based Imputation (GAIN / GRUI)"}),  (l:Library {name:"gluonts"})     CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Sliding Window"}),                      (l:Library {name:"numpy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Expanding Window (Walk-Forward) CV"}),  (l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Sliding Window Cross-Validation"}),     (l:Library {name:"scikit-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Temporal Downsampling"}),               (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Temporal Upsampling"}),                 (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Jittering (Gaussian Noise Addition)"}), (l:Library {name:"tsaug"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Magnitude Scaling"}),                   (l:Library {name:"tsaug"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Time Warping"}),                        (l:Library {name:"tsaug"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Window Slicing"}),                      (l:Library {name:"tsaug"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"DTW Barycentric Averaging"}),           (l:Library {name:"dtaidistance"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"GAN-Based Augmentation (TimeGAN / TGAN)"}),(l:Library {name:"ydata-synthetic"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Frequency Domain Augmentation"}),       (l:Library {name:"numpy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"SMOTE for Time Series"}),               (l:Library {name:"imbalanced-learn"}) CREATE (t)-[:IMPLEMENTED_IN]->(l);

MATCH (t:Technique {name:"Lag Features"}),                        (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Rolling Statistical Features"}),        (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"EWM Features"}),                        (l:Library {name:"pandas"})      CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"FFT / Spectral Features"}),             (l:Library {name:"numpy"})       CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Wavelet Transform Features"}),          (l:Library {name:"PyWavelets"})  CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"tsfresh Automated Feature Extraction"}),(l:Library {name:"tsfresh"})     CREATE (t)-[:IMPLEMENTED_IN]->(l);
MATCH (t:Technique {name:"Change Point Detection"}),              (l:Library {name:"ruptures"})    CREATE (t)-[:IMPLEMENTED_IN]->(l);

// ─────────────────────────────────────────────────────────────────────────────
// MODEL → IMPLEMENTED_IN Library
// ─────────────────────────────────────────────────────────────────────────────
MATCH (m:Model {name:"AR (Autoregressive)"}),                    (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"MA (Moving Average)"}),                    (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"ARIMA"}),                                  (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"SARIMA"}),                                 (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"ARIMAX / SARIMAX"}),                       (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Exponential Smoothing (ETS)"}),            (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"VAR (Vector Autoregression)"}),            (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Theta Method"}),                           (l:Library {name:"statsmodels"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Prophet"}),                                (l:Library {name:"prophet"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"XGBoost"}),                                (l:Library {name:"XGBoost"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"LightGBM"}),                               (l:Library {name:"LightGBM"})          CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Random Forest"}),                          (l:Library {name:"scikit-learn"})      CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Support Vector Regression / SVC"}),        (l:Library {name:"scikit-learn"})      CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"RNN (Vanilla Recurrent Neural Network)"}), (l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"RNN (Vanilla Recurrent Neural Network)"}), (l:Library {name:"TensorFlow/Keras"})  CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),          (l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),          (l:Library {name:"TensorFlow/Keras"})  CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),             (l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),             (l:Library {name:"TensorFlow/Keras"})  CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"1D-CNN (Temporal Convolutional Network)"}),(l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"1D-CNN (Temporal Convolutional Network)"}),(l:Library {name:"TensorFlow/Keras"})  CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(l:Library {name:"PyTorch"}) CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Transformer (Self-Attention)"}),           (l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Informer / Autoformer / PatchTST"}),       (l:Library {name:"neuralforecast"})    CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"N-BEATS"}),                                (l:Library {name:"neuralforecast"})    CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Temporal Fusion Transformer (TFT)"}),      (l:Library {name:"neuralforecast"})    CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"DeepAR"}),                                 (l:Library {name:"gluonts"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"TimesNet"}),                               (l:Library {name:"PyTorch"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Isolation Forest"}),                       (l:Library {name:"scikit-learn"})      CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Local Outlier Factor"}),                   (l:Library {name:"scikit-learn"})      CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}),(l:Library {name:"PyTorch"})       CREATE (m)-[:IMPLEMENTED_IN]->(l);
MATCH (m:Model {name:"Spectral Residual (SR) + CNN"}),           (l:Library {name:"merlion"})           CREATE (m)-[:IMPLEMENTED_IN]->(l);

// ─────────────────────────────────────────────────────────────────────────────
// USE CASE → APPLIES PredictionType + Model
// ─────────────────────────────────────────────────────────────────────────────
MATCH (u:UseCase {name:"Predictive Maintenance"}),     (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Predictive Maintenance"}),     (p:PredictionType {name:"Anomaly Detection"})                     CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Predictive Maintenance"}),     (p:PredictionType {name:"Time Series Classification"})            CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Energy Demand Forecasting"}),  (p:PredictionType {name:"Time Series Forecasting"})               CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Financial Time Series"}),      (p:PredictionType {name:"Time Series Forecasting"})               CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Financial Time Series"}),      (p:PredictionType {name:"Anomaly Detection"})                     CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Financial Time Series"}),      (p:PredictionType {name:"Change Point Detection"})                CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"IoT Sensor Monitoring"}),      (p:PredictionType {name:"Anomaly Detection"})                     CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"IoT Sensor Monitoring"}),      (p:PredictionType {name:"Time Series Imputation"})                CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Healthcare / Clinical TS"}),   (p:PredictionType {name:"Time Series Classification"})            CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Healthcare / Clinical TS"}),   (p:PredictionType {name:"Anomaly Detection"})                     CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Retail Sales Forecasting"}),   (p:PredictionType {name:"Time Series Forecasting"})               CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Weather / Climate Forecasting"}),(p:PredictionType {name:"Time Series Forecasting"})             CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Network Anomaly Detection"}),  (p:PredictionType {name:"Anomaly Detection"})                     CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Network Anomaly Detection"}),  (p:PredictionType {name:"Time Series Classification"})            CREATE (u)-[:APPLIES]->(p);
MATCH (u:UseCase {name:"Prognostics & Health Management (PHM)"}),(p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (u)-[:APPLIES]->(p);

// Best Model for Use Case
MATCH (u:UseCase {name:"Predictive Maintenance"}),     (m:Model {name:"LSTM (Long Short-Term Memory)"})        CREATE (u)-[:RECOMMENDED_MODEL {reason:"Captures degradation trajectory over long sequences"}]->(m);
MATCH (u:UseCase {name:"Predictive Maintenance"}),     (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}) CREATE (u)-[:RECOMMENDED_MODEL {reason:"Fast, parallelisable, strong benchmark on CMAPSS"}]->(m);
MATCH (u:UseCase {name:"Energy Demand Forecasting"}),  (m:Model {name:"Temporal Fusion Transformer (TFT)"})    CREATE (u)-[:RECOMMENDED_MODEL {reason:"Handles known-future covariates like calendar and weather"}]->(m);
MATCH (u:UseCase {name:"Retail Sales Forecasting"}),   (m:Model {name:"LightGBM"})                             CREATE (u)-[:RECOMMENDED_MODEL {reason:"Won M5 competition; handles promotions and categorical features"}]->(m);
MATCH (u:UseCase {name:"Financial Time Series"}),      (m:Model {name:"ARIMA"})                                CREATE (u)-[:RECOMMENDED_MODEL {reason:"Baseline; interpretable; fast"}]->(m);
MATCH (u:UseCase {name:"IoT Sensor Monitoring"}),      (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}) CREATE (u)-[:RECOMMENDED_MODEL {reason:"Trained on normal; detects novel anomalies without labels"}]->(m);

// ─────────────────────────────────────────────────────────────────────────────
// BEST PRACTICE / ANTI-PATTERN → APPLIES_TO PipelineStage
// ─────────────────────────────────────────────────────────────────────────────
MATCH (b:BestPractice {name:"Always Use Temporal Train/Test Split"}),         (s:PipelineStage {name:"Sampling & Windowing"})        CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Fit Scalers/Encoders Only on Training Data"}),   (s:PipelineStage {name:"Preprocessing"})               CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Establish a Naïve Baseline First"}),             (s:PipelineStage {name:"Modelling"})                   CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Perform Residual Diagnostics"}),                 (s:PipelineStage {name:"Evaluation"})                  CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Report Prediction Intervals, Not Just Point Forecasts"}),(s:PipelineStage {name:"Evaluation"})          CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Use Time Series Cross-Validation"}),             (s:PipelineStage {name:"Sampling & Windowing"})        CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Use Early Stopping for DL Models"}),             (s:PipelineStage {name:"Modelling"})                   CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Monitor for Data and Concept Drift in Production"}),(s:PipelineStage {name:"Deployment & Monitoring"})  CREATE (b)-[:APPLIES_TO]->(s);
MATCH (b:BestPractice {name:"Incorporate Domain Knowledge in Feature Engineering"}),(s:PipelineStage {name:"Feature Engineering"})   CREATE (b)-[:APPLIES_TO]->(s);

MATCH (a:AntiPattern {name:"Random Train/Test Split on Time Series"}),        (s:PipelineStage {name:"Sampling & Windowing"})        CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Fit Scaler on Full Dataset Before Split"}),       (s:PipelineStage {name:"Preprocessing"})               CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Running ARIMA on Non-Stationary Series Without Differencing"}),(s:PipelineStage {name:"Modelling"})      CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Applying SMOTE Before Train/Test Split"}),        (s:PipelineStage {name:"Data Augmentation"})           CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Aggressive Smoothing Before Anomaly Detection"}), (s:PipelineStage {name:"Preprocessing"})               CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Including Future-Derived Features"}),             (s:PipelineStage {name:"Feature Engineering"})         CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Evaluating Only on Accuracy or RMSE"}),           (s:PipelineStage {name:"Evaluation"})                  CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Look-Ahead Bias in Window Features"}),            (s:PipelineStage {name:"Feature Engineering"})         CREATE (a)-[:WARNS_ABOUT]->(s);
MATCH (a:AntiPattern {name:"Using Future Data in Validation Fold Features"}), (s:PipelineStage {name:"Sampling & Windowing"})        CREATE (a)-[:WARNS_ABOUT]->(s);

// ─────────────────────────────────────────────────────────────────────────────
// CONCEPT → REQUIRES understanding of other Concepts (PREREQUISITE chain)
// ─────────────────────────────────────────────────────────────────────────────
MATCH (a:Concept {name:"Stationarity"}),        (b:Concept {name:"Trend"})                   CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Stationarity"}),        (b:Concept {name:"Seasonality"})             CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Stationarity"}),        (b:Concept {name:"Unit Root"})               CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Autocorrelation"}),     (b:Concept {name:"Temporal Dependency"})     CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Data Leakage"}),        (b:Concept {name:"Temporal Dependency"})     CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Remaining Useful Life (RUL)"}),(b:Concept {name:"Change Point"})     CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Concept Drift"}),       (b:Concept {name:"Stationarity"})            CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);
MATCH (a:Concept {name:"Granger Causality"}),   (b:Concept {name:"Autocorrelation"})         CREATE (a)-[:REQUIRES_UNDERSTANDING]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// MODEL COMPARISON edges
// ─────────────────────────────────────────────────────────────────────────────
MATCH (a:Model {name:"LSTM (Long Short-Term Memory)"}),    (b:Model {name:"GRU (Gated Recurrent Unit)"})
CREATE (a)-[:COMPARED_TO {
    difference: "LSTM has 3 gates vs GRU 2 gates",
    lstm_strength: "Slightly better on very long sequences",
    gru_strength: "Faster training; fewer parameters; often equivalent accuracy"
}]->(b);

MATCH (a:Model {name:"ARIMA"}),    (b:Model {name:"Exponential Smoothing (ETS)"})
CREATE (a)-[:COMPARED_TO {
    difference: "ARIMA is ARMA on differenced series; ETS is state-space",
    arima_strength: "More flexible autocorrelation modelling via MA terms",
    ets_strength: "Simpler; more robust on short series; fewer hyperparameters"
}]->(b);

MATCH (a:Model {name:"XGBoost"}),  (b:Model {name:"LightGBM"})
CREATE (a)-[:COMPARED_TO {
    difference: "LightGBM uses leaf-wise splitting vs XGBoost level-wise",
    xgb_strength: "More stable with less data; better regularisation",
    lgbm_strength: "10x faster on large datasets; lower memory"
}]->(b);

MATCH (a:Model {name:"LSTM (Long Short-Term Memory)"}),  (b:Model {name:"Transformer (Self-Attention)"})
CREATE (a)-[:COMPARED_TO {
    difference: "Transformer uses attention over all time steps; LSTM is sequential",
    lstm_strength: "More memory-efficient on very long sequences; simpler",
    transformer_strength: "Parallelisable; captures long-range dependencies without vanishing gradient"
}]->(b);

MATCH (a:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),
      (b:Model {name:"LSTM (Long Short-Term Memory)"})
CREATE (a)-[:COMPARED_TO {
    difference: "TCN is parallelisable; LSTM is sequential",
    tcn_strength: "Faster training; dilated convs for long receptive field; causal by design",
    lstm_strength: "Variable-length sequences; better suited for online/streaming inference"
}]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// CONCEPT → RELEVANT_TO PredictionType
// ─────────────────────────────────────────────────────────────────────────────
MATCH (c:Concept {name:"Stationarity"}),          (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Seasonality"}),           (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Trend"}),                 (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Data Leakage"}),          (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Forecast Horizon"}),      (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Lookback Window"}),       (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Remaining Useful Life (RUL)"}),(p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Anomaly Types"}),         (p:PredictionType {name:"Anomaly Detection"})                   CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Class Imbalance in Time Series"}),(p:PredictionType {name:"Anomaly Detection"})           CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Class Imbalance in Time Series"}),(p:PredictionType {name:"Time Series Classification"})  CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Change Point"}),          (p:PredictionType {name:"Change Point Detection"})              CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Concept Drift"}),         (p:PredictionType {name:"Time Series Forecasting"})             CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Temporal Dependency"}),   (p:PredictionType {name:"Time Series Classification"})          CREATE (c)-[:RELEVANT_TO]->(p);
MATCH (c:Concept {name:"Multivariate Time Series"}),(p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (c)-[:RELEVANT_TO]->(p);

// ─────────────────────────────────────────────────────────────────────────────
// ANTI-PATTERN → VIOLATES BestPractice
// ─────────────────────────────────────────────────────────────────────────────
MATCH (a:AntiPattern {name:"Random Train/Test Split on Time Series"}),       (b:BestPractice {name:"Always Use Temporal Train/Test Split"})   CREATE (a)-[:VIOLATES]->(b);
MATCH (a:AntiPattern {name:"Fit Scaler on Full Dataset Before Split"}),      (b:BestPractice {name:"Fit Scalers/Encoders Only on Training Data"}) CREATE (a)-[:VIOLATES]->(b);
MATCH (a:AntiPattern {name:"Applying SMOTE Before Train/Test Split"}),       (b:BestPractice {name:"Always Use Temporal Train/Test Split"})   CREATE (a)-[:VIOLATES]->(b);
MATCH (a:AntiPattern {name:"Skipping the Naïve Baseline"}),                  (b:BestPractice {name:"Establish a Naïve Baseline First"})       CREATE (a)-[:VIOLATES]->(b);
MATCH (a:AntiPattern {name:"Evaluating Only on Accuracy or RMSE"}),          (b:BestPractice {name:"Report Prediction Intervals, Not Just Point Forecasts"}) CREATE (a)-[:VIOLATES]->(b);
MATCH (a:AntiPattern {name:"Look-Ahead Bias in Window Features"}),           (b:BestPractice {name:"Fit Scalers/Encoders Only on Training Data"}) CREATE (a)-[:VIOLATES]->(b);

// ═══════════════════════════════════════════════════════════════════════════════
// PART 3: HIERARCHICAL ENHANCEMENTS
// New relationship types:
//   NEXT_STAGE        — explicit pipeline chain between PipelineStage nodes
//   MUST_PRECEDE      — within-stage technique ordering
//   LEARN_BEFORE      — concept learning progression
//   REQUIRES_CONCEPT  — technique/model prereqs
//   HAS_STEP          — LearningPath → ordered steps
// ═══════════════════════════════════════════════════════════════════════════════

// ─────────────────────────────────────────────────────────────────────────────
// 1. PIPELINE STAGE CHAIN  — NEXT_STAGE (traversable directed chain)
// ─────────────────────────────────────────────────────────────────────────────
MATCH (a:PipelineStage {name:"Data Ingestion"}),           (b:PipelineStage {name:"Exploratory Data Analysis"})  CREATE (a)-[:NEXT_STAGE {step:1}]->(b);
MATCH (a:PipelineStage {name:"Exploratory Data Analysis"}),(b:PipelineStage {name:"Preprocessing"})               CREATE (a)-[:NEXT_STAGE {step:2}]->(b);
MATCH (a:PipelineStage {name:"Preprocessing"}),            (b:PipelineStage {name:"Sampling & Windowing"})        CREATE (a)-[:NEXT_STAGE {step:3}]->(b);
MATCH (a:PipelineStage {name:"Sampling & Windowing"}),     (b:PipelineStage {name:"Data Augmentation"})           CREATE (a)-[:NEXT_STAGE {step:4}]->(b);
MATCH (a:PipelineStage {name:"Data Augmentation"}),        (b:PipelineStage {name:"Feature Engineering"})         CREATE (a)-[:NEXT_STAGE {step:5}]->(b);
MATCH (a:PipelineStage {name:"Feature Engineering"}),      (b:PipelineStage {name:"Modelling"})                   CREATE (a)-[:NEXT_STAGE {step:6}]->(b);
MATCH (a:PipelineStage {name:"Modelling"}),                (b:PipelineStage {name:"Evaluation"})                  CREATE (a)-[:NEXT_STAGE {step:7}]->(b);
MATCH (a:PipelineStage {name:"Evaluation"}),               (b:PipelineStage {name:"Deployment & Monitoring"})     CREATE (a)-[:NEXT_STAGE {step:8}]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// 2. WITHIN-STAGE TECHNIQUE ORDERING — MUST_PRECEDE
// ─────────────────────────────────────────────────────────────────────────────

// EDA sub-chain: visualise first → assess missingness → stationarity tests → autocorrelation → decompose → outliers
MATCH (a:Technique {name:"Time Series Line Plot"}),          (b:Technique {name:"Missing Value Pattern Analysis"})       CREATE (a)-[:MUST_PRECEDE {reason:"Understand raw shape before assessing missingness"}]->(b);
MATCH (a:Technique {name:"Missing Value Pattern Analysis"}), (b:Technique {name:"Rolling Mean & Std Dev"})                CREATE (a)-[:MUST_PRECEDE {reason:"Only assess stationarity after understanding gaps"}]->(b);
MATCH (a:Technique {name:"Rolling Mean & Std Dev"}),         (b:Technique {name:"Augmented Dickey-Fuller Test"})          CREATE (a)-[:MUST_PRECEDE {reason:"Visual stationarity check before formal test"}]->(b);
MATCH (a:Technique {name:"Augmented Dickey-Fuller Test"}),   (b:Technique {name:"KPSS Test"})                             CREATE (a)-[:MUST_PRECEDE {reason:"ADF and KPSS together confirm stationarity decision"}]->(b);
MATCH (a:Technique {name:"KPSS Test"}),                      (b:Technique {name:"ACF Plot"})                              CREATE (a)-[:MUST_PRECEDE {reason:"Confirm stationarity before reading ACF meaningfully"}]->(b);
MATCH (a:Technique {name:"ACF Plot"}),                       (b:Technique {name:"PACF Plot"})                             CREATE (a)-[:MUST_PRECEDE {reason:"ACF guides MA order; PACF guides AR order — read together"}]->(b);
MATCH (a:Technique {name:"PACF Plot"}),                      (b:Technique {name:"Seasonal Decomposition"})                CREATE (a)-[:MUST_PRECEDE {reason:"Seasonal decomposition builds on autocorrelation insight"}]->(b);
MATCH (a:Technique {name:"Seasonal Decomposition"}),         (b:Technique {name:"STL Decomposition"})                    CREATE (a)-[:MUST_PRECEDE {reason:"Use classical decomp as baseline before STL"}]->(b);
MATCH (a:Technique {name:"STL Decomposition"}),              (b:Technique {name:"Change Point Detection"})               CREATE (a)-[:MUST_PRECEDE {reason:"Decompose trend/season before identifying structural breaks"}]->(b);
MATCH (a:Technique {name:"Change Point Detection"}),         (b:Technique {name:"IQR Outlier Detection"})                CREATE (a)-[:MUST_PRECEDE {reason:"Detect regime changes before labelling outliers within regimes"}]->(b);
MATCH (a:Technique {name:"IQR Outlier Detection"}),          (b:Technique {name:"Z-Score Outlier Detection"})            CREATE (a)-[:MUST_PRECEDE {reason:"Use IQR first (robust), then Z-score as cross-check"}]->(b);
MATCH (a:Technique {name:"Z-Score Outlier Detection"}),      (b:Technique {name:"Granger Causality Test"})               CREATE (a)-[:MUST_PRECEDE {reason:"Clean data before testing inter-series causality"}]->(b);

// Preprocessing sub-chain: resampling → alignment → outlier handling → imputation → scaling → smoothing → transforms → detrend → deseason
MATCH (a:Technique {name:"Resampling / Frequency Alignment"}), (b:Technique {name:"Time Index Alignment"})              CREATE (a)-[:MUST_PRECEDE {reason:"Standardise frequency before aligning multiple sources"}]->(b);
MATCH (a:Technique {name:"Time Index Alignment"}),             (b:Technique {name:"Outlier Clipping / Winsorisation"})  CREATE (a)-[:MUST_PRECEDE {reason:"Align indices before handling outliers"}]->(b);
MATCH (a:Technique {name:"Outlier Clipping / Winsorisation"}), (b:Technique {name:"Forward Fill (Last Observation Carried Forward)"}) CREATE (a)-[:MUST_PRECEDE {reason:"Cap extreme values before filling gaps"}]->(b);
MATCH (a:Technique {name:"Forward Fill (Last Observation Carried Forward)"}), (b:Technique {name:"Linear Interpolation"}) CREATE (a)-[:MUST_PRECEDE {reason:"Simple fill for short gaps; interpolation for longer ones"}]->(b);
MATCH (a:Technique {name:"Linear Interpolation"}),             (b:Technique {name:"Min-Max Scaling"})                   CREATE (a)-[:MUST_PRECEDE {reason:"Impute missing values before scaling; scaling assumes complete data"}]->(b);
MATCH (a:Technique {name:"Min-Max Scaling"}),                  (b:Technique {name:"Z-Score Standardisation"})          CREATE (a)-[:MUST_PRECEDE {reason:"Choose one scaler; Z-score is alternative to Min-Max"}]->(b);
MATCH (a:Technique {name:"Z-Score Standardisation"}),          (b:Technique {name:"Robust Scaling"})                   CREATE (a)-[:MUST_PRECEDE {reason:"Robust scaling is preferred over Z-score when outliers remain"}]->(b);
MATCH (a:Technique {name:"Robust Scaling"}),                   (b:Technique {name:"Moving Average Smoothing"})         CREATE (a)-[:MUST_PRECEDE {reason:"Scale before smoothing to prevent magnitude-driven window distortion"}]->(b);
MATCH (a:Technique {name:"Moving Average Smoothing"}),         (b:Technique {name:"Log Transform"})                    CREATE (a)-[:MUST_PRECEDE {reason:"Smooth noise; then variance-stabilise with log if multiplicative"}]->(b);
MATCH (a:Technique {name:"Log Transform"}),                    (b:Technique {name:"Box-Cox Transform"})                CREATE (a)-[:MUST_PRECEDE {reason:"Log is special case of Box-Cox; confirm lambda before using Box-Cox"}]->(b);
MATCH (a:Technique {name:"Box-Cox Transform"}),                (b:Technique {name:"Differencing"})                     CREATE (a)-[:MUST_PRECEDE {reason:"Stabilise variance first; then difference to achieve mean stationarity"}]->(b);
MATCH (a:Technique {name:"Differencing"}),                     (b:Technique {name:"Detrending"})                       CREATE (a)-[:MUST_PRECEDE {reason:"First-order differencing removes stochastic trend; detrending for deterministic trend"}]->(b);
MATCH (a:Technique {name:"Detrending"}),                       (b:Technique {name:"Deseasoning"})                      CREATE (a)-[:MUST_PRECEDE {reason:"Remove trend before seasonal adjustment"}]->(b);

// Imputation sub-chain: simple fills → interpolation → model-based
MATCH (a:Technique {name:"Forward Fill (Last Observation Carried Forward)"}), (b:Technique {name:"Backward Fill"})     CREATE (a)-[:MUST_PRECEDE {reason:"FFill for real-time causal imputation; BFill only for non-causal historical correction"}]->(b);
MATCH (a:Technique {name:"Backward Fill"}),                    (b:Technique {name:"Spline Interpolation"})             CREATE (a)-[:MUST_PRECEDE {reason:"Try simple fills before complex spline interpolation"}]->(b);
MATCH (a:Technique {name:"Spline Interpolation"}),             (b:Technique {name:"Seasonal Mean Imputation"})         CREATE (a)-[:MUST_PRECEDE {reason:"Use seasonal mean when spline fails over season-length gaps"}]->(b);
MATCH (a:Technique {name:"Seasonal Mean Imputation"}),         (b:Technique {name:"Kalman Filter Imputation"})         CREATE (a)-[:MUST_PRECEDE {reason:"Kalman is more complex; use after simpler methods fail"}]->(b);
MATCH (a:Technique {name:"Kalman Filter Imputation"}),         (b:Technique {name:"MICE (Multiple Imputation by Chained Equations)"}) CREATE (a)-[:MUST_PRECEDE {reason:"MICE for MAR multivariate case after univariate methods exhausted"}]->(b);
MATCH (a:Technique {name:"MICE (Multiple Imputation by Chained Equations)"}), (b:Technique {name:"GAN-Based Imputation (GAIN / GRUI)"}) CREATE (a)-[:MUST_PRECEDE {reason:"GAN imputation is last resort for long complex gaps; needs large data"}]->(b);

// Sampling sub-chain: split strategy first → windowing → cross-validation
MATCH (a:Technique {name:"Sliding Window"}),                   (b:Technique {name:"Tumbling (Non-Overlapping) Window"}) CREATE (a)-[:MUST_PRECEDE {reason:"Understand overlapping windows before choosing non-overlapping variant"}]->(b);
MATCH (a:Technique {name:"Tumbling (Non-Overlapping) Window"}),(b:Technique {name:"Expanding Window (Walk-Forward) CV"}) CREATE (a)-[:MUST_PRECEDE {reason:"Define base windows before applying cross-validation scheme"}]->(b);
MATCH (a:Technique {name:"Expanding Window (Walk-Forward) CV"}),(b:Technique {name:"Sliding Window Cross-Validation"})  CREATE (a)-[:MUST_PRECEDE {reason:"Expanding CV is baseline; sliding CV refines with fixed training size"}]->(b);

// Augmentation sub-chain: simple noise → scaling → temporal → generative
MATCH (a:Technique {name:"Jittering (Gaussian Noise Addition)"}), (b:Technique {name:"Magnitude Scaling"})             CREATE (a)-[:MUST_PRECEDE {reason:"Apply simple augmentations first; assess diversity before complex methods"}]->(b);
MATCH (a:Technique {name:"Magnitude Scaling"}),                (b:Technique {name:"Time Warping"})                     CREATE (a)-[:MUST_PRECEDE {reason:"Amplitude augmentation before temporal distortion"}]->(b);
MATCH (a:Technique {name:"Time Warping"}),                     (b:Technique {name:"Window Slicing"})                   CREATE (a)-[:MUST_PRECEDE {reason:"Temporal augmentation before sub-sequence extraction"}]->(b);
MATCH (a:Technique {name:"Window Slicing"}),                   (b:Technique {name:"Time Series Mixup"})                CREATE (a)-[:MUST_PRECEDE {reason:"Deterministic augmentations before label-interpolating mixup"}]->(b);
MATCH (a:Technique {name:"Time Series Mixup"}),                (b:Technique {name:"DTW Barycentric Averaging"})        CREATE (a)-[:MUST_PRECEDE {reason:"Linear mixup before DTW-aware averaging"}]->(b);
MATCH (a:Technique {name:"DTW Barycentric Averaging"}),        (b:Technique {name:"SMOTE for Time Series"})            CREATE (a)-[:MUST_PRECEDE {reason:"DTW averaging before SMOTE for minority oversampling"}]->(b);
MATCH (a:Technique {name:"SMOTE for Time Series"}),            (b:Technique {name:"Frequency Domain Augmentation"})   CREATE (a)-[:MUST_PRECEDE {reason:"Standard oversampling before frequency-space perturbation"}]->(b);
MATCH (a:Technique {name:"Frequency Domain Augmentation"}),    (b:Technique {name:"GAN-Based Augmentation (TimeGAN / TGAN)"}) CREATE (a)-[:MUST_PRECEDE {reason:"Deterministic augmentations before complex generative GAN training"}]->(b);

// Feature Engineering sub-chain: lag → rolling → EWM → spectral → calendar → automated → target encoding
MATCH (a:Technique {name:"Lag Features"}),                     (b:Technique {name:"Rolling Statistical Features"})     CREATE (a)-[:MUST_PRECEDE {reason:"Direct lags before aggregated rolling statistics"}]->(b);
MATCH (a:Technique {name:"Rolling Statistical Features"}),     (b:Technique {name:"EWM Features"})                     CREATE (a)-[:MUST_PRECEDE {reason:"Uniform rolling before exponentially-weighted variants"}]->(b);
MATCH (a:Technique {name:"EWM Features"}),                     (b:Technique {name:"Cumulative / Integrated Features"}) CREATE (a)-[:MUST_PRECEDE {reason:"Local features before global cumulative features"}]->(b);
MATCH (a:Technique {name:"Cumulative / Integrated Features"}), (b:Technique {name:"Calendar / Temporal Features"})     CREATE (a)-[:MUST_PRECEDE {reason:"Time-domain features before external calendar encoding"}]->(b);
MATCH (a:Technique {name:"Calendar / Temporal Features"}),     (b:Technique {name:"FFT / Spectral Features"})          CREATE (a)-[:MUST_PRECEDE {reason:"Temporal context before frequency-domain features"}]->(b);
MATCH (a:Technique {name:"FFT / Spectral Features"}),          (b:Technique {name:"Wavelet Transform Features"})       CREATE (a)-[:MUST_PRECEDE {reason:"FFT (global frequency) before Wavelet (time-frequency localisation)"}]->(b);
MATCH (a:Technique {name:"Wavelet Transform Features"}),       (b:Technique {name:"tsfresh Automated Feature Extraction"}) CREATE (a)-[:MUST_PRECEDE {reason:"Manual domain features before automated extraction to guide feature selection"}]->(b);
MATCH (a:Technique {name:"tsfresh Automated Feature Extraction"}),(b:Technique {name:"Target Encoding of Categorical Covariates"}) CREATE (a)-[:MUST_PRECEDE {reason:"Extract numeric features before encoding categorical covariates"}]->(b);

// Modelling progression: naive baseline → statistical → ML → deep learning
MATCH (a:Model {name:"AR (Autoregressive)"}),                  (b:Model {name:"MA (Moving Average)"})                  CREATE (a)-[:MUST_PRECEDE {reason:"AR captures own-lag structure; MA captures error structure — understand separately"}]->(b);
MATCH (a:Model {name:"MA (Moving Average)"}),                  (b:Model {name:"ARIMA"})                                CREATE (a)-[:MUST_PRECEDE {reason:"ARIMA combines AR and MA; must understand components first"}]->(b);
MATCH (a:Model {name:"ARIMA"}),                                (b:Model {name:"SARIMA"})                               CREATE (a)-[:MUST_PRECEDE {reason:"SARIMA extends ARIMA with seasonal terms"}]->(b);
MATCH (a:Model {name:"SARIMA"}),                               (b:Model {name:"Exponential Smoothing (ETS)"})          CREATE (a)-[:MUST_PRECEDE {reason:"Compare SARIMA with ETS as state-space alternative"}]->(b);
MATCH (a:Model {name:"Exponential Smoothing (ETS)"}),          (b:Model {name:"Prophet"})                              CREATE (a)-[:MUST_PRECEDE {reason:"ETS is classical baseline; Prophet extends to trend changepoints and holidays"}]->(b);
MATCH (a:Model {name:"Prophet"}),                              (b:Model {name:"XGBoost"})                              CREATE (a)-[:MUST_PRECEDE {reason:"Statistical models first; then ML tree models which require manual lag features"}]->(b);
MATCH (a:Model {name:"XGBoost"}),                              (b:Model {name:"LightGBM"})                             CREATE (a)-[:MUST_PRECEDE {reason:"XGBoost is reference; LightGBM is faster alternative — compare both"}]->(b);
MATCH (a:Model {name:"LightGBM"}),                             (b:Model {name:"Random Forest"})                        CREATE (a)-[:MUST_PRECEDE {reason:"Boosting then bagging comparison"}]->(b);
MATCH (a:Model {name:"Random Forest"}),                        (b:Model {name:"LSTM (Long Short-Term Memory)"})        CREATE (a)-[:MUST_PRECEDE {reason:"ML baselines before deep learning; DL only justified if ML falls short"}]->(b);
MATCH (a:Model {name:"LSTM (Long Short-Term Memory)"}),        (b:Model {name:"GRU (Gated Recurrent Unit)"})           CREATE (a)-[:MUST_PRECEDE {reason:"LSTM is reference RNN; GRU is leaner variant to compare"}]->(b);
MATCH (a:Model {name:"GRU (Gated Recurrent Unit)"}),           (b:Model {name:"1D-CNN (Temporal Convolutional Network)"}) CREATE (a)-[:MUST_PRECEDE {reason:"Recurrent models before parallel convolutional temporal models"}]->(b);
MATCH (a:Model {name:"1D-CNN (Temporal Convolutional Network)"}),(b:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}) CREATE (a)-[:MUST_PRECEDE {reason:"Standard 1D CNN before dilated causal TCN"}]->(b);
MATCH (a:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(b:Model {name:"Transformer (Self-Attention)"}) CREATE (a)-[:MUST_PRECEDE {reason:"CNN-based temporal model before attention-based; attention is more data-hungry"}]->(b);
MATCH (a:Model {name:"Transformer (Self-Attention)"}),         (b:Model {name:"Temporal Fusion Transformer (TFT)"})    CREATE (a)-[:MUST_PRECEDE {reason:"Standard Transformer before TFT which adds covariate handling and quantile output"}]->(b);
MATCH (a:Model {name:"Temporal Fusion Transformer (TFT)"}),    (b:Model {name:"Informer / Autoformer / PatchTST"})     CREATE (a)-[:MUST_PRECEDE {reason:"TFT before efficient Transformer variants for long-sequence tasks"}]->(b);
MATCH (a:Model {name:"Informer / Autoformer / PatchTST"}),     (b:Model {name:"N-BEATS"})                              CREATE (a)-[:MUST_PRECEDE {reason:"Attention-based models before pure FC residual stack (N-BEATS)"}]->(b);
MATCH (a:Model {name:"N-BEATS"}),                              (b:Model {name:"DeepAR"})                               CREATE (a)-[:MUST_PRECEDE {reason:"Deterministic DL before probabilistic autoregressive DL"}]->(b);
MATCH (a:Model {name:"DeepAR"}),                               (b:Model {name:"TimesNet"})                             CREATE (a)-[:MUST_PRECEDE {reason:"Established DL models before newer 2D-transformation approach"}]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// 3. CONCEPT LEARNING PROGRESSION — LEARN_BEFORE
// ─────────────────────────────────────────────────────────────────────────────

// Foundational temporal concepts
MATCH (a:Concept {name:"Temporal Dependency"}),       (b:Concept {name:"Autocorrelation"})                  CREATE (a)-[:LEARN_BEFORE {reason:"Temporal dependency is the intuition behind autocorrelation"}]->(b);
MATCH (a:Concept {name:"Autocorrelation"}),           (b:Concept {name:"Partial Autocorrelation"})          CREATE (a)-[:LEARN_BEFORE {reason:"PACF extends ACF by removing shorter-lag influence"}]->(b);
MATCH (a:Concept {name:"Partial Autocorrelation"}),   (b:Concept {name:"Multicollinearity in Lagged Features"}) CREATE (a)-[:LEARN_BEFORE {reason:"PACF reveals multicollinearity in lag feature space"}]->(b);

// Decomposition hierarchy
MATCH (a:Concept {name:"Trend"}),                     (b:Concept {name:"Seasonality"})                      CREATE (a)-[:LEARN_BEFORE {reason:"Trend is the simplest non-stationarity; seasonality is periodic non-stationarity"}]->(b);
MATCH (a:Concept {name:"Seasonality"}),               (b:Concept {name:"Cyclicality"})                      CREATE (a)-[:LEARN_BEFORE {reason:"Distinguish fixed-period seasonality from irregular cyclicality"}]->(b);
MATCH (a:Concept {name:"Cyclicality"}),               (b:Concept {name:"Irregular / Noise"})                CREATE (a)-[:LEARN_BEFORE {reason:"Understand all signal components before characterising residual noise"}]->(b);
MATCH (a:Concept {name:"Trend"}),                     (b:Concept {name:"Stationarity"})                     CREATE (a)-[:LEARN_BEFORE {reason:"Trend and seasonality are the main stationarity violators"}]->(b);
MATCH (a:Concept {name:"Seasonality"}),               (b:Concept {name:"Stationarity"})                     CREATE (a)-[:LEARN_BEFORE {reason:"Understand seasonal patterns before stationarity testing"}]->(b);
MATCH (a:Concept {name:"Stationarity"}),              (b:Concept {name:"Unit Root"})                        CREATE (a)-[:LEARN_BEFORE {reason:"Unit root is the formal statistical definition of non-stationarity"}]->(b);
MATCH (a:Concept {name:"Unit Root"}),                 (b:Concept {name:"Granger Causality"})                CREATE (a)-[:LEARN_BEFORE {reason:"Spurious Granger causality arises from non-stationary (unit root) series"}]->(b);

// Forecasting concepts
MATCH (a:Concept {name:"Forecast Horizon"}),          (b:Concept {name:"Lookback Window"})                  CREATE (a)-[:LEARN_BEFORE {reason:"Horizon defines the prediction target; lookback defines the input window"}]->(b);
MATCH (a:Concept {name:"Lookback Window"}),           (b:Concept {name:"Data Leakage"})                     CREATE (a)-[:LEARN_BEFORE {reason:"Understanding windows makes temporal leakage risks concrete"}]->(b);
MATCH (a:Concept {name:"Temporal Dependency"}),       (b:Concept {name:"Data Leakage"})                     CREATE (a)-[:LEARN_BEFORE {reason:"Temporal dependency is why random splits cause leakage"}]->(b);
MATCH (a:Concept {name:"Data Leakage"}),              (b:Concept {name:"Concept Drift"})                    CREATE (a)-[:LEARN_BEFORE {reason:"Leakage inflates scores; drift deflates production scores — both distort evaluation"}]->(b);

// Anomaly and fault concepts
MATCH (a:Concept {name:"Anomaly Types"}),             (b:Concept {name:"Class Imbalance in Time Series"})   CREATE (a)-[:LEARN_BEFORE {reason:"Anomaly types define the labelling challenge; imbalance is its consequence"}]->(b);
MATCH (a:Concept {name:"Change Point"}),              (b:Concept {name:"Concept Drift"})                    CREATE (a)-[:LEARN_BEFORE {reason:"Change points are discrete; concept drift is the continuous generalisation"}]->(b);
MATCH (a:Concept {name:"Stationarity"}),              (b:Concept {name:"Concept Drift"})                    CREATE (a)-[:LEARN_BEFORE {reason:"Concept drift is a generalised non-stationarity in production context"}]->(b);

// Multivariate and covariate concepts
MATCH (a:Concept {name:"Autocorrelation"}),           (b:Concept {name:"Exogenous Variables / Covariates"}) CREATE (a)-[:LEARN_BEFORE {reason:"Understand auto-dependency before adding cross-series dependency"}]->(b);
MATCH (a:Concept {name:"Exogenous Variables / Covariates"}), (b:Concept {name:"Multivariate Time Series"})  CREATE (a)-[:LEARN_BEFORE {reason:"Covariates are exogenous inputs; multivariate TS models them jointly"}]->(b);
MATCH (a:Concept {name:"Irregular Sampling"}),        (b:Concept {name:"Multivariate Time Series"})         CREATE (a)-[:LEARN_BEFORE {reason:"Irregular sampling compounds in multivariate settings; understand it first"}]->(b);

// Scaling and normalisation
MATCH (a:Concept {name:"Stationarity"}),              (b:Concept {name:"Normalization / Scaling"})          CREATE (a)-[:LEARN_BEFORE {reason:"Scaling assumptions relate to distributional properties studied under stationarity"}]->(b);

// RUL / PHM chain
MATCH (a:Concept {name:"Change Point"}),              (b:Concept {name:"Remaining Useful Life (RUL)"})      CREATE (a)-[:LEARN_BEFORE {reason:"RUL computation starts from degradation onset — a change point"}]->(b);
MATCH (a:Concept {name:"Remaining Useful Life (RUL)"}),(b:Concept {name:"Anomaly Types"})                   CREATE (a)-[:LEARN_BEFORE {reason:"Understand RUL before connecting to fault/anomaly classification"}]->(b);

// ─────────────────────────────────────────────────────────────────────────────
// 4. TECHNIQUE / MODEL → REQUIRES_CONCEPT  (prerequisite concepts for usage)
// ─────────────────────────────────────────────────────────────────────────────

// EDA techniques
MATCH (t:Technique {name:"ACF Plot"}),                         (c:Concept {name:"Autocorrelation"})          CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"PACF Plot"}),                        (c:Concept {name:"Partial Autocorrelation"})  CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"PACF Plot"}),                        (c:Concept {name:"Autocorrelation"})          CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Augmented Dickey-Fuller Test"}),     (c:Concept {name:"Stationarity"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Augmented Dickey-Fuller Test"}),     (c:Concept {name:"Unit Root"})                CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"KPSS Test"}),                        (c:Concept {name:"Stationarity"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Seasonal Decomposition"}),           (c:Concept {name:"Seasonality"})              CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Seasonal Decomposition"}),           (c:Concept {name:"Trend"})                    CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"STL Decomposition"}),                (c:Concept {name:"Seasonality"})              CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"STL Decomposition"}),                (c:Concept {name:"Trend"})                    CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Granger Causality Test"}),           (c:Concept {name:"Granger Causality"})        CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Granger Causality Test"}),           (c:Concept {name:"Stationarity"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Change Point Detection"}),           (c:Concept {name:"Change Point"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);

// Preprocessing techniques
MATCH (t:Technique {name:"Differencing"}),                     (c:Concept {name:"Stationarity"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Differencing"}),                     (c:Concept {name:"Unit Root"})                CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Detrending"}),                       (c:Concept {name:"Trend"})                    CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Deseasoning"}),                      (c:Concept {name:"Seasonality"})              CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Min-Max Scaling"}),                  (c:Concept {name:"Normalization / Scaling"})  CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Z-Score Standardisation"}),          (c:Concept {name:"Normalization / Scaling"})  CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Robust Scaling"}),                   (c:Concept {name:"Normalization / Scaling"})  CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Resampling / Frequency Alignment"}), (c:Concept {name:"Irregular Sampling"})       CREATE (t)-[:REQUIRES_CONCEPT]->(c);

// Sampling techniques
MATCH (t:Technique {name:"Sliding Window"}),                   (c:Concept {name:"Lookback Window"})          CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Sliding Window"}),                   (c:Concept {name:"Forecast Horizon"})         CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Sliding Window"}),                   (c:Concept {name:"Data Leakage"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Expanding Window (Walk-Forward) CV"}),(c:Concept {name:"Temporal Dependency"})     CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Expanding Window (Walk-Forward) CV"}),(c:Concept {name:"Data Leakage"})            CREATE (t)-[:REQUIRES_CONCEPT]->(c);

// Augmentation techniques
MATCH (t:Technique {name:"SMOTE for Time Series"}),            (c:Concept {name:"Class Imbalance in Time Series"}) CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"GAN-Based Augmentation (TimeGAN / TGAN)"}),(c:Concept {name:"Class Imbalance in Time Series"}) CREATE (t)-[:REQUIRES_CONCEPT]->(c);

// Feature engineering techniques
MATCH (t:Technique {name:"Lag Features"}),                     (c:Concept {name:"Autocorrelation"})          CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Lag Features"}),                     (c:Concept {name:"Temporal Dependency"})      CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Lag Features"}),                     (c:Concept {name:"Data Leakage"})             CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Rolling Statistical Features"}),     (c:Concept {name:"Lookback Window"})          CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"FFT / Spectral Features"}),          (c:Concept {name:"Seasonality"})              CREATE (t)-[:REQUIRES_CONCEPT]->(c);
MATCH (t:Technique {name:"Cumulative / Integrated Features"}), (c:Concept {name:"Remaining Useful Life (RUL)"}) CREATE (t)-[:REQUIRES_CONCEPT]->(c);

// Statistical models
MATCH (m:Model {name:"ARIMA"}),                                (c:Concept {name:"Stationarity"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"ARIMA"}),                                (c:Concept {name:"Autocorrelation"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"ARIMA"}),                                (c:Concept {name:"Partial Autocorrelation"})  CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"ARIMA"}),                                (c:Concept {name:"Unit Root"})                CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"SARIMA"}),                               (c:Concept {name:"Seasonality"})              CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"SARIMA"}),                               (c:Concept {name:"Stationarity"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Exponential Smoothing (ETS)"}),          (c:Concept {name:"Trend"})                    CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Exponential Smoothing (ETS)"}),          (c:Concept {name:"Seasonality"})              CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"VAR (Vector Autoregression)"}),          (c:Concept {name:"Multivariate Time Series"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"VAR (Vector Autoregression)"}),          (c:Concept {name:"Granger Causality"})        CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"VAR (Vector Autoregression)"}),          (c:Concept {name:"Stationarity"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);

// ML models
MATCH (m:Model {name:"XGBoost"}),                              (c:Concept {name:"Lookback Window"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"XGBoost"}),                              (c:Concept {name:"Data Leakage"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"LightGBM"}),                             (c:Concept {name:"Lookback Window"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"LightGBM"}),                             (c:Concept {name:"Data Leakage"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);

// Deep learning models
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),        (c:Concept {name:"Lookback Window"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),        (c:Concept {name:"Temporal Dependency"})      CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"LSTM (Long Short-Term Memory)"}),        (c:Concept {name:"Normalization / Scaling"})  CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),           (c:Concept {name:"Lookback Window"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"GRU (Gated Recurrent Unit)"}),           (c:Concept {name:"Temporal Dependency"})      CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(c:Concept {name:"Lookback Window"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"}),(c:Concept {name:"Temporal Dependency"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Transformer (Self-Attention)"}),         (c:Concept {name:"Lookback Window"})          CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Transformer (Self-Attention)"}),         (c:Concept {name:"Temporal Dependency"})      CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Temporal Fusion Transformer (TFT)"}),    (c:Concept {name:"Exogenous Variables / Covariates"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Temporal Fusion Transformer (TFT)"}),    (c:Concept {name:"Forecast Horizon"})         CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"DeepAR"}),                               (c:Concept {name:"Multivariate Time Series"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);

// Anomaly detection models
MATCH (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}),(c:Concept {name:"Anomaly Types"})       CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"}),(c:Concept {name:"Lookback Window"})     CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Isolation Forest"}),                     (c:Concept {name:"Anomaly Types"})             CREATE (m)-[:REQUIRES_CONCEPT]->(c);
MATCH (m:Model {name:"Isolation Forest"}),                     (c:Concept {name:"Class Imbalance in Time Series"}) CREATE (m)-[:REQUIRES_CONCEPT]->(c);

// ─────────────────────────────────────────────────────────────────────────────
// 5. LEARNING PATH NODES + HAS_STEP chains
// ─────────────────────────────────────────────────────────────────────────────

CREATE (lpForecasting:LearningPath {
    name: "Forecasting Fundamentals Path",
    goal: "Go from zero to building and evaluating a production-ready univariate forecasting model",
    target_audience: "Data scientists new to time series",
    estimated_steps: 14
})

CREATE (lpAnomaly:LearningPath {
    name: "Anomaly Detection Path",
    goal: "Detect point, contextual, and collective anomalies in sensor and operational time series",
    target_audience: "ML engineers working on IoT, industrial, or cybersecurity data",
    estimated_steps: 13
})

CREATE (lpDeepLearning:LearningPath {
    name: "Deep Learning for Time Series Path",
    goal: "Build and tune LSTM, TCN, and Transformer models for forecasting, classification, and RUL",
    target_audience: "Practitioners with ML experience looking to apply DL to sequential data",
    estimated_steps: 15
})

CREATE (lpRUL:LearningPath {
    name: "Predictive Maintenance & RUL Path",
    goal: "End-to-end pipeline for industrial degradation modelling and remaining useful life prediction",
    target_audience: "Engineers in manufacturing, aerospace, or energy domains",
    estimated_steps: 12
});

// ── Forecasting Fundamentals Path ────────────────────────────────────────────
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (c:Concept {name:"Temporal Dependency"})
  CREATE (lp)-[:HAS_STEP {step:1, type:"Concept", note:"Foundation: why time series are different from tabular data"}]->(c);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (c:Concept {name:"Trend"})
  CREATE (lp)-[:HAS_STEP {step:2, type:"Concept", note:"Understand the long-term direction component"}]->(c);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (c:Concept {name:"Seasonality"})
  CREATE (lp)-[:HAS_STEP {step:3, type:"Concept", note:"Understand recurring periodic patterns"}]->(c);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (c:Concept {name:"Stationarity"})
  CREATE (lp)-[:HAS_STEP {step:4, type:"Concept", note:"Core assumption of classical models"}]->(c);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (c:Concept {name:"Autocorrelation"})
  CREATE (lp)-[:HAS_STEP {step:5, type:"Concept", note:"Quantify temporal self-correlation"}]->(c);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"Time Series Line Plot"})
  CREATE (lp)-[:HAS_STEP {step:6, type:"Technique", note:"First EDA: visualise the series"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"Augmented Dickey-Fuller Test"})
  CREATE (lp)-[:HAS_STEP {step:7, type:"Technique", note:"Test for stationarity formally"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"ACF Plot"})
  CREATE (lp)-[:HAS_STEP {step:8, type:"Technique", note:"Identify seasonality lags and MA order"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"PACF Plot"})
  CREATE (lp)-[:HAS_STEP {step:9, type:"Technique", note:"Identify AR order for ARIMA"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"Differencing"})
  CREATE (lp)-[:HAS_STEP {step:10, type:"Technique", note:"Transform to stationarity"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"Sliding Window"})
  CREATE (lp)-[:HAS_STEP {step:11, type:"Technique", note:"Create supervised (X, y) samples"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (t:Technique {name:"Expanding Window (Walk-Forward) CV"})
  CREATE (lp)-[:HAS_STEP {step:12, type:"Technique", note:"Honest temporal cross-validation"}]->(t);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (m:Model {name:"ARIMA"})
  CREATE (lp)-[:HAS_STEP {step:13, type:"Model", note:"First real model: ARIMA on stationary series"}]->(m);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}), (m:Model {name:"XGBoost"})
  CREATE (lp)-[:HAS_STEP {step:14, type:"Model", note:"Upgrade to tree-based ML with lag features"}]->(m);

// ── Anomaly Detection Path ────────────────────────────────────────────────────
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (c:Concept {name:"Temporal Dependency"})
  CREATE (lp)-[:HAS_STEP {step:1, type:"Concept", note:"Anomaly detection requires temporal context to distinguish normal from abnormal"}]->(c);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (c:Concept {name:"Anomaly Types"})
  CREATE (lp)-[:HAS_STEP {step:2, type:"Concept", note:"Learn point vs contextual vs collective anomalies"}]->(c);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (c:Concept {name:"Class Imbalance in Time Series"})
  CREATE (lp)-[:HAS_STEP {step:3, type:"Concept", note:"Anomalies are rare — understand class imbalance impact"}]->(c);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (c:Concept {name:"Change Point"})
  CREATE (lp)-[:HAS_STEP {step:4, type:"Concept", note:"Distinguish point anomalies from structural change points"}]->(c);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"Time Series Line Plot"})
  CREATE (lp)-[:HAS_STEP {step:5, type:"Technique", note:"Visual inspection to spot obvious anomalies"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"Rolling Mean & Std Dev"})
  CREATE (lp)-[:HAS_STEP {step:6, type:"Technique", note:"Track local distribution shifts"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"IQR Outlier Detection"})
  CREATE (lp)-[:HAS_STEP {step:7, type:"Technique", note:"Statistical baseline for flagging outliers"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"Change Point Detection"})
  CREATE (lp)-[:HAS_STEP {step:8, type:"Technique", note:"Structural break detection with ruptures"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"Sliding Window"})
  CREATE (lp)-[:HAS_STEP {step:9, type:"Technique", note:"Create context windows for model input"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (t:Technique {name:"SMOTE for Time Series"})
  CREATE (lp)-[:HAS_STEP {step:10, type:"Technique", note:"Handle minority class — only after split"}]->(t);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (m:Model {name:"Isolation Forest"})
  CREATE (lp)-[:HAS_STEP {step:11, type:"Model", note:"Unsupervised baseline — no labels needed"}]->(m);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (m:Model {name:"Autoencoder / LSTM-AE for Anomaly Detection"})
  CREATE (lp)-[:HAS_STEP {step:12, type:"Model", note:"Reconstruction-error anomaly detection"}]->(m);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}), (m:Model {name:"Spectral Residual (SR) + CNN"})
  CREATE (lp)-[:HAS_STEP {step:13, type:"Model", note:"Frequency-domain unsupervised anomaly detection"}]->(m);

// ── Deep Learning for Time Series Path ───────────────────────────────────────
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (c:Concept {name:"Lookback Window"})
  CREATE (lp)-[:HAS_STEP {step:1, type:"Concept", note:"DL requires fixed-size input windows — understand this design decision"}]->(c);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (c:Concept {name:"Temporal Dependency"})
  CREATE (lp)-[:HAS_STEP {step:2, type:"Concept", note:"Why sequential models outperform standard MLPs on time series"}]->(c);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (c:Concept {name:"Normalization / Scaling"})
  CREATE (lp)-[:HAS_STEP {step:3, type:"Concept", note:"DL is sensitive to scale — always normalise inputs"}]->(c);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (c:Concept {name:"Data Leakage"})
  CREATE (lp)-[:HAS_STEP {step:4, type:"Concept", note:"DL pipelines have subtle leakage risks — understand all forms"}]->(c);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (t:Technique {name:"Min-Max Scaling"})
  CREATE (lp)-[:HAS_STEP {step:5, type:"Technique", note:"Scale inputs to [0,1] for RNN stability"}]->(t);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (t:Technique {name:"Sliding Window"})
  CREATE (lp)-[:HAS_STEP {step:6, type:"Technique", note:"Create (X, y) tensor pairs from sequential data"}]->(t);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (t:Technique {name:"Expanding Window (Walk-Forward) CV"})
  CREATE (lp)-[:HAS_STEP {step:7, type:"Technique", note:"Temporal CV to avoid leaking future into validation"}]->(t);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"LSTM (Long Short-Term Memory)"})
  CREATE (lp)-[:HAS_STEP {step:8, type:"Model", note:"Start with LSTM — canonical RNN for time series"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"GRU (Gated Recurrent Unit)"})
  CREATE (lp)-[:HAS_STEP {step:9, type:"Model", note:"Compare GRU as leaner LSTM alternative"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"1D-CNN (Temporal Convolutional Network)"})
  CREATE (lp)-[:HAS_STEP {step:10, type:"Model", note:"Parallel convolution as alternative to recurrence"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"})
  CREATE (lp)-[:HAS_STEP {step:11, type:"Model", note:"Dilated causal convolutions for large receptive fields"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"Transformer (Self-Attention)"})
  CREATE (lp)-[:HAS_STEP {step:12, type:"Model", note:"Attention-based model for long-range dependencies"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"Temporal Fusion Transformer (TFT)"})
  CREATE (lp)-[:HAS_STEP {step:13, type:"Model", note:"TFT for multi-horizon forecasting with covariates"}]->(m);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (t:Technique {name:"Jittering (Gaussian Noise Addition)"})
  CREATE (lp)-[:HAS_STEP {step:14, type:"Technique", note:"Augmentation to prevent DL overfitting"}]->(t);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}), (m:Model {name:"TimesNet"})
  CREATE (lp)-[:HAS_STEP {step:15, type:"Model", note:"State-of-the-art 2D transformation approach — final frontier"}]->(m);

// ── Predictive Maintenance & RUL Path ────────────────────────────────────────
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (c:Concept {name:"Temporal Dependency"})
  CREATE (lp)-[:HAS_STEP {step:1, type:"Concept", note:"Degradation is inherently temporal — sequential memory matters"}]->(c);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (c:Concept {name:"Change Point"})
  CREATE (lp)-[:HAS_STEP {step:2, type:"Concept", note:"Fault onset is a change point — detect it accurately"}]->(c);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (c:Concept {name:"Remaining Useful Life (RUL)"})
  CREATE (lp)-[:HAS_STEP {step:3, type:"Concept", note:"Understand how RUL is defined and labelled from run-to-failure data"}]->(c);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (c:Concept {name:"Class Imbalance in Time Series"})
  CREATE (lp)-[:HAS_STEP {step:4, type:"Concept", note:"Failure events are rare — must handle imbalance in fault classification"}]->(c);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (c:Concept {name:"Multivariate Time Series"})
  CREATE (lp)-[:HAS_STEP {step:5, type:"Concept", note:"Sensors produce multivariate streams — model cross-channel dependencies"}]->(c);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (t:Technique {name:"Missing Value Pattern Analysis"})
  CREATE (lp)-[:HAS_STEP {step:6, type:"Technique", note:"Sensor dropout is common in industrial data — characterise gaps first"}]->(t);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (t:Technique {name:"Kalman Filter Imputation"})
  CREATE (lp)-[:HAS_STEP {step:7, type:"Technique", note:"Model-based imputation for noisy sensor streams"}]->(t);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (t:Technique {name:"Rolling Statistical Features"})
  CREATE (lp)-[:HAS_STEP {step:8, type:"Technique", note:"Rolling mean/std capture degradation trend over time"}]->(t);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (t:Technique {name:"Cumulative / Integrated Features"})
  CREATE (lp)-[:HAS_STEP {step:9, type:"Technique", note:"Cumulative operation cycles track total wear"}]->(t);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (t:Technique {name:"Wavelet Transform Features"})
  CREATE (lp)-[:HAS_STEP {step:10, type:"Technique", note:"Time-frequency features for vibration-based fault detection"}]->(t);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (m:Model {name:"LSTM (Long Short-Term Memory)"})
  CREATE (lp)-[:HAS_STEP {step:11, type:"Model", note:"LSTM captures long degradation trajectory"}]->(m);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}), (m:Model {name:"TCN (Temporal Convolutional Network with Dilated Causal Convolutions)"})
  CREATE (lp)-[:HAS_STEP {step:12, type:"Model", note:"TCN: strong CMAPSS benchmark performer; parallelisable"}]->(m);

// ─────────────────────────────────────────────────────────────────────────────
// LearningPath → COVERS PredictionType
// ─────────────────────────────────────────────────────────────────────────────
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}),        (p:PredictionType {name:"Time Series Forecasting"})               CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}),               (p:PredictionType {name:"Anomaly Detection"})                     CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}),               (p:PredictionType {name:"Change Point Detection"})                CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}),   (p:PredictionType {name:"Time Series Forecasting"})               CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}),   (p:PredictionType {name:"Time Series Classification"})            CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}),   (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}),    (p:PredictionType {name:"Remaining Useful Life (RUL) Prediction"}) CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}),    (p:PredictionType {name:"Anomaly Detection"})                     CREATE (lp)-[:COVERS]->(p);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}),    (p:PredictionType {name:"Prognostics & Health Management (PHM)"}) CREATE (lp)-[:COVERS]->(p);

// LearningPath → RECOMMENDED_FOR UseCase
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}),        (u:UseCase {name:"Energy Demand Forecasting"})   CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}),        (u:UseCase {name:"Retail Sales Forecasting"})    CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Forecasting Fundamentals Path"}),        (u:UseCase {name:"Weather / Climate Forecasting"}) CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}),               (u:UseCase {name:"IoT Sensor Monitoring"})       CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}),               (u:UseCase {name:"Network Anomaly Detection"})   CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Anomaly Detection Path"}),               (u:UseCase {name:"Financial Time Series"})       CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Deep Learning for Time Series Path"}),   (u:UseCase {name:"Healthcare / Clinical TS"})    CREATE (lp)-[:RECOMMENDED_FOR]->(u);
MATCH (lp:LearningPath {name:"Predictive Maintenance & RUL Path"}),    (u:UseCase {name:"Predictive Maintenance"})      CREATE (lp)-[:RECOMMENDED_FOR]->(u);

